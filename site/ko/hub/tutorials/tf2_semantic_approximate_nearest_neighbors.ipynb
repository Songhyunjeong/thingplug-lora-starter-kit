{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACbjNjyO4f_8"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCM50vaM4jiK"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOVy-_vmuUP"
      },
      "source": [
        "# Approximate Nearest Neighbor(ANN) 및 텍스트 임베딩을 사용한 의미론적 검색\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_semantic_approximate_nearest_neighbors\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf2_semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a>\n",
        "</td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/hub/tutorials/tf2_semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 보기</a>\n",
        "</td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/hub/tutorials/tf2_semantic_approximate_nearest_neighbors.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운론드하기</a></td>\n",
        "  <td><a href=\"https://tfhub.dev/google/nnlm-en-dim128/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\">TF 허브 모델 보기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T4d77AJaKte"
      },
      "source": [
        "이 튜토리얼에서는 입력 데이터가 제공된 [TensorFlow Hub](https://tfhub.dev)(TF-Hub) 모듈에서 임베딩을 생성하고 추출된 임베딩을 사용하여 approximate nearest neighbour(ANN) 인덱스를 빌드하는 방법을 보여줍니다. 그런 다음 이 인덱스를 실시간 유사성 일치 및 검색에 사용할 수 있습니다.\n",
        "\n",
        "많은 양의 데이터를 처리할 때 전체 리포지토리를 스캔하여 주어진 쿼리와 가장 유사한 항목을 실시간으로 찾는 식으로 정확한 일치 작업을 수행하는 것은 효율적이지 않습니다. 따라서 속도를 크게 높이기 위해 정확한 nearest neighbor(NN) 일치를 찾을 때 약간의 정확성을 절충할 수 있는 근사 유사성 일치 알고리즘을 사용합니다.\n",
        "\n",
        "이 튜토리얼에서는 쿼리와 가장 유사한 헤드라인을 찾기 위해 뉴스 헤드라인 자료의 텍스트를 실시간으로 검색하는 예를 보여줍니다. 키워드 검색과 달리 이 검색으로 텍스트 임베딩에 인코딩된 의미론적 유사성이 포착됩니다.\n",
        "\n",
        "이 튜토리얼의 단계는 다음과 같습니다.\n",
        "\n",
        "1. 샘플 데이터를 다운로드합니다.\n",
        "2. TF-Hub 모델을 사용하여 데이터에 대한 임베딩을 생성합니다.\n",
        "3. 임베딩의 ANN 인덱스를 빌드합니다.\n",
        "4. 유사성 일치에 인덱스를 사용합니다.\n",
        "\n",
        "[Apache Beam](https://beam.apache.org/documentation/programming-guide/)을 사용하여 TF-Hub 모델에서 임베딩을 생성합니다. 또한 Spotify의 [ANNOY](https://github.com/spotify/annoy) 라이브러리를 사용하여 근사적으로 최근접 인덱스를 빌드합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM17v_mEVSnd"
      },
      "source": [
        "### 더 많은 모델\n",
        "\n",
        "아키텍처는 동일하지만 다른 언어로 훈련된 모델의 경우, [이](https://tfhub.dev/google/collections/nnlm/1) 컬렉션을 참조하세요. [여기](https://tfhub.dev/s?module-type=text-embedding)에서 현재 [tfhub.dev](https://tfhub.dev/)에서 호스팅되는 모든 텍스트 임베딩을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0jr0QK9qO5P"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whMRj9qeqed4"
      },
      "source": [
        "필요한 라이브러리를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qmXkLPoaqS--",
        "outputId": "360eb3d1-046d-44c4-bf0e-e29f0b8d272b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache_beam\n",
            "  Downloading apache_beam-2.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache_beam)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (3.10.10)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle~=2.2.1 (from apache_beam)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache_beam)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache_beam)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (1.64.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache_beam)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (3.3.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (1.26.4)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache_beam)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (24.1)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache_beam)\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (3.20.3)\n",
            "Collecting pydot<2,>=1.2.0 (from apache_beam)\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (2024.2)\n",
            "Collecting redis<6,>=5.0.0 (from apache_beam)\n",
            "  Downloading redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (2024.9.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (4.12.2)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache_beam)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow<17.0.0,>=3.0.0 (from apache_beam)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache_beam) (0.6)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache_beam)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.20.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache_beam)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache_beam) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2024.8.30)\n",
            "Downloading apache_beam-2.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.2.0-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: crcmod, dill, hdfs, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31405 sha256=af194ccd384487308e4246671b7b9116570350c43c98cdc10c51ba957d436dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=c804a99b070e4a91e0faa51055ae316519edbcd8fbd6e33eb4fd0a642a483ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=a1f60c4c43f1b8a1671cb4093499939211d11a1443ea6e2de88d94fa8fbc7896\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=03ac58db900baa452db83a5a71b7b8430a3884704885b3b04091cf7f088fe695\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built crcmod dill hdfs docopt\n",
            "Installing collected packages: docopt, crcmod, zstandard, redis, pydot, pyarrow, objsize, fasteners, fastavro, dnspython, dill, cloudpickle, pymongo, hdfs, apache_beam\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.2\n",
            "    Uninstalling pydot-3.0.2:\n",
            "      Successfully uninstalled pydot-3.0.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.0\n",
            "    Uninstalling cloudpickle-3.1.0:\n",
            "      Successfully uninstalled cloudpickle-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache_beam-2.60.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docopt-0.6.2 fastavro-1.9.7 fasteners-0.19 hdfs-2.7.3 objsize-0.7.0 pyarrow-16.1.0 pydot-1.4.2 pymongo-4.10.1 redis-5.2.0 zstandard-0.23.0\n",
            "Collecting scikit_learn~=0.23.0\n",
            "  Downloading scikit-learn-0.23.2.tar.gz (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=552447 sha256=ca52a203370b212daa513624885543f80abedf19283276596901ad287a0427a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.3\n"
          ]
        }
      ],
      "source": [
        "!pip install apache_beam\n",
        "!pip install 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.\n",
        "!pip install annoy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-vBZiCCqld0"
      },
      "source": [
        "필요한 라이브러리를 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6NTYbdWcseuK",
        "outputId": "6cb6f62a-ca71-4d22-a1ef-45f129c79ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'gaussian_random_matrix' from 'sklearn.random_projection' (/usr/local/lib/python3.10/dist-packages/sklearn/random_projection.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4dd2b2359329>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannoy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_projection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_random_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'gaussian_random_matrix' from 'sklearn.random_projection' (/usr/local/lib/python3.10/dist-packages/sklearn/random_projection.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import apache_beam as beam\n",
        "from apache_beam.transforms import util\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import annoy\n",
        "from sklearn.random_projection import gaussian_random_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tx0SZa6-7b-f",
        "outputId": "4eec4189-357e-45d9-aa4c-70d56fd16f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.17.0\n",
            "TF-Hub version: 0.16.1\n",
            "Apache Beam version: 2.60.0\n"
          ]
        }
      ],
      "source": [
        "print('TF version: {}'.format(tf.__version__))\n",
        "print('TF-Hub version: {}'.format(hub.__version__))\n",
        "print('Apache Beam version: {}'.format(beam.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Imq876rLWx"
      },
      "source": [
        "## 1. 샘플 데이터를 다운로드합니다.\n",
        "\n",
        "[A Million News Headlines](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL#) 데이터세트에는 평판이 좋은 Australian Broadcasting Corp. (ABC)에서 공급한 15년치의 뉴스 헤드라인이 수록되어 있습니다. 이 뉴스 데이터세트에는 호주에 보다 세분화된 초점을 두고 2003년 초부터 2017년 말까지 전 세계적으로 일어난 주목할만한 사건에 대한 역사적 기록이 요약되어 있습니다.\n",
        "\n",
        "**형식**: 탭으로 구분된 2열 데이터: 1) 발행일 및 2) 헤드라인 텍스트. 여기서는 헤드라인 텍스트에만 관심이 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OpF57n8e5C9D",
        "outputId": "b0d1eb28-13d4-4b1d-aa12-6d23d5f0e66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-07 02:12:01--  https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true\n",
            "Resolving dataverse.harvard.edu (dataverse.harvard.edu)... 3.208.36.246, 3.223.176.15, 3.228.243.207\n",
            "Connecting to dataverse.harvard.edu (dataverse.harvard.edu)|3.208.36.246|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57600231 (55M) [text/tab-separated-values]\n",
            "Saving to: ‘raw.tsv’\n",
            "\n",
            "raw.tsv             100%[===================>]  54.93M  48.1MB/s    in 1.1s    \n",
            "\n",
            "2024-11-07 02:12:03 (48.1 MB/s) - ‘raw.tsv’ saved [57600231/57600231]\n",
            "\n",
            "1103664 raw.tsv\n",
            "publish_date\theadline_text\n",
            "20030219\t\"aba decides against community broadcasting licence\"\n",
            "20030219\t\"act fire witnesses must be aware of defamation\"\n",
            "20030219\t\"a g calls for infrastructure protection summit\"\n",
            "20030219\t\"air nz staff in aust strike for pay rise\"\n",
            "20030219\t\"air nz strike to affect australian travellers\"\n",
            "20030219\t\"ambitious olsson wins triple jump\"\n",
            "20030219\t\"antic delighted with record breaking barca\"\n",
            "20030219\t\"aussie qualifier stosur wastes four memphis match\"\n",
            "20030219\t\"aust addresses un security council over iraq\"\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://dataverse.harvard.edu/api/access/datafile/3450625?format=tab&gbrecs=true' -O raw.tsv\n",
        "!wc -l raw.tsv\n",
        "!head raw.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reeoc9z0zTxJ"
      },
      "source": [
        "단순화를 위해 헤드라인 텍스트만 유지하고 발행일은 제거합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/sample_data/민원분류.csv')\n",
        "df1=df['UpdatedTitle']\n",
        "df1.to_csv('/content/sample_data/민원분류_title.csv',index=False)\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "-BFI-oCPliyZ",
        "outputId": "d6541e58-c7fe-4d72-eccf-45aa5f7f107e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         수리요청바랍니다\n",
              "1    안녕하세요 여기는. 오정동 우체국 옆 상가 주차장 입\n",
              "2     쓰레기기를  낮애버려놓아 더니기불편해요* 안전신문고\n",
              "3      표디판이 기울어져 있어요바르게 다시 설치요청* 안\n",
              "4                        주차 차단봉 설치\n",
              "Name: UpdatedTitle, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UpdatedTitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>수리요청바랍니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>안녕하세요 여기는. 오정동 우체국 옆 상가 주차장 입</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>쓰레기기를  낮애버려놓아 더니기불편해요* 안전신문고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>표디판이 기울어져 있어요바르게 다시 설치요청* 안</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>주차 차단봉 설치</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "INPWa4upv_yJ"
      },
      "outputs": [],
      "source": [
        "!rm -r corpus\n",
        "!mkdir corpus\n",
        "\n",
        "with open('corpus/text.txt', 'w') as out_file:\n",
        "  with open('/content/sample_data/민원분류_title.csv', 'r') as in_file:\n",
        "    for line in in_file:\n",
        "      headline = line.split('\\t')[0].strip().strip('\"')\n",
        "      out_file.write(headline+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5-oedX40z6o2",
        "outputId": "5fa2f63e-f146-4cb4-b25c-8b636f14d1e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수리 부탁드립니다 * 안전신문고 신고파일(사진·동영\n",
            "브레이크등 미작동\n",
            "수리 부탁드립니다 * 안전신문고 신고파일(사진·동영\n",
            "가로등 수리 부탁드립니다 * 안전신문고 신고파일(사\n",
            "가로등 수리 부탁드립니다 * 안전신문고 신고파일(사\n",
            "가로등 수리 부탁드립니다 * 안전신문고 신고파일(사\n",
            "가로등 수리 부탁드립니다 * 안전신문고 신고파일(사\n",
            "브레이크등 미작동\n",
            "수리 부탁 드리겠습니다* 안전신문고 신고파일(사진·\n",
            "번호등 불량 제보합니다차량 소유주에게 안내 부탁드립니\n"
          ]
        }
      ],
      "source": [
        "!tail corpus/text.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AngMtH50jNb"
      },
      "source": [
        "## 2. 데이터에 대한 임베딩 생성하기\n",
        "\n",
        "이 튜토리얼에서는 [Neural Network Language Model(NNLM)](https://tfhub.dev/google/nnlm-en-dim128/2)를 사용하여 헤드라인 데이터에 대한 임베딩을 생성합니다. 그런 다음 문장 임베딩을 사용하여 문장 수준의 의미 유사성을 쉽게 계산할 수 있습니다. Apache Beam을 사용하여 임베딩 생성 프로세스를 실행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_DvXnDB1pEX"
      },
      "source": [
        "### 임베딩 추출 메서드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yL7OEY1E0A35"
      },
      "outputs": [],
      "source": [
        "embed_fn = None\n",
        "\n",
        "def generate_embeddings(text, model_url, random_projection_matrix=None):\n",
        "  # Beam will run this function in different processes that need to\n",
        "  # import hub and load embed_fn (if not previously loaded)\n",
        "  global embed_fn\n",
        "  if embed_fn is None:\n",
        "    embed_fn = hub.load(model_url)\n",
        "  embedding = embed_fn(text).numpy()\n",
        "  if random_projection_matrix is not None:\n",
        "    embedding = embedding.dot(random_projection_matrix)\n",
        "  return text, embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6pXBVxsVUbm"
      },
      "source": [
        "### tf.Example 메서드로 변환하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JMjqjWZNVVzd"
      },
      "outputs": [],
      "source": [
        "def to_tf_example(entries):\n",
        "  examples = []\n",
        "\n",
        "  text_list, embedding_list = entries\n",
        "  for i in range(len(text_list)):\n",
        "    text = text_list[i]\n",
        "    embedding = embedding_list[i]\n",
        "\n",
        "    features = {\n",
        "        'text': tf.train.Feature(\n",
        "            bytes_list=tf.train.BytesList(value=[text.encode('utf-8')])),\n",
        "        'embedding': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(value=embedding.tolist()))\n",
        "    }\n",
        "\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature=features)).SerializeToString(deterministic=True)\n",
        "\n",
        "    examples.append(example)\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDiV4uQCVYGH"
      },
      "source": [
        "### Beam 파이프라인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jCGUIB172m2G"
      },
      "outputs": [],
      "source": [
        "def run_hub2emb(args):\n",
        "  '''Runs the embedding generation pipeline'''\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
        "\n",
        "  with beam.Pipeline(args.runner, options=options) as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | 'Read sentences from files' >> beam.io.ReadFromText(\n",
        "            file_pattern=args.data_dir)\n",
        "        | 'Batch elements' >> util.BatchElements(\n",
        "            min_batch_size=args.batch_size, max_batch_size=args.batch_size)\n",
        "        | 'Generate embeddings' >> beam.Map(\n",
        "            generate_embeddings, args.model_url, args.random_projection_matrix)\n",
        "        | 'Encode to tf example' >> beam.FlatMap(to_tf_example)\n",
        "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
        "            file_path_prefix='{}/emb'.format(args.output_dir),\n",
        "            file_name_suffix='.tfrecords')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbQdiYNVvne"
      },
      "source": [
        "### 무작위 투영 가중치 행렬 생성하기\n",
        "\n",
        "[무작위 투영](https://en.wikipedia.org/wiki/Random_projection)은 유클리드 공간에 있는 점 집합의 차원을 줄이는 데 사용되는 간단하지만 강력한 기술입니다. 이론적 배경은 [Johnson-Lindenstrauss 보조 정리](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)를 참조하세요.\n",
        "\n",
        "무작위 투영으로 임베딩의 차원을 줄이면 ANN 인덱스를 빌드하고 쿼리하는 데 필요한 시간이 줄어듭니다.\n",
        "\n",
        "이 튜토리얼에서는 [Scikit-learn](https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection) 라이브러리의 [가우스 무작위 투영](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection)을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1yw1xgtNVv52"
      },
      "outputs": [],
      "source": [
        "def generate_random_projection_weights(original_dim, projected_dim):\n",
        "  random_projection_matrix = None\n",
        "  random_projection_matrix = gaussian_random_matrix(\n",
        "      n_components=projected_dim, n_features=original_dim).T\n",
        "  print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
        "  print('Storing random projection matrix to disk...')\n",
        "  with open('random_projection_matrix', 'wb') as handle:\n",
        "    pickle.dump(random_projection_matrix,\n",
        "                handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return random_projection_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJZUfT3NE7kj"
      },
      "source": [
        "### 매개변수 설정하기\n",
        "\n",
        "무작위 투영 없이 원래 임베딩 공간을 사용하여 인덱스를 빌드하려면 `projected_dim` 매개변수를 `None`으로 설정합니다. 그러면 높은 차원의 임베딩에 대한 인덱싱 스텝이 느려집니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_projection_weights(original_dim, projected_dim):\n",
        "    # Gaussian 랜덤 행렬 생성\n",
        "    random_projection_matrix = gaussian_random_matrix(\n",
        "        n_components=projected_dim, n_features=original_dim).T\n",
        "    print(\"A Gaussian random weight matrix was created with shape of {}\".format(random_projection_matrix.shape))\n",
        "    return random_projection_matrix"
      ],
      "metadata": {
        "id": "kxypEIokcG0T"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "cellView": "form",
        "id": "77-Cow7uE74T"
      },
      "outputs": [],
      "source": [
        "model_url = 'https://tfhub.dev/google/nnlm-en-dim128/2' #@param {type:\"string\"}\n",
        "projected_dim = 64  #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On-MbzD922kb"
      },
      "source": [
        "### 파이프라인 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Y3I1Wv4i21yY",
        "outputId": "1d158e47-1df4-44a6-d9ca-c97f76b2d058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Gaussian random weight matrix was created with shape of (128, 64)\n",
            "Pipeline args are set.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_name': 'hub2emb-241107-031452',\n",
              " 'runner': 'DirectRunner',\n",
              " 'batch_size': 1024,\n",
              " 'data_dir': 'corpus/*.txt',\n",
              " 'output_dir': '/tmp/tmpex0u2i5l',\n",
              " 'model_url': 'https://tfhub.dev/google/nnlm-en-dim128/2',\n",
              " 'random_projection_matrix': array([[ 0.0018381 , -0.03032105, -0.18692303, ..., -0.02025734,\n",
              "         -0.12891474, -0.0048825 ],\n",
              "        [-0.14750635, -0.09745011, -0.03123811, ..., -0.18726707,\n",
              "         -0.05538086,  0.0398117 ],\n",
              "        [ 0.01147445, -0.12545985,  0.13579665, ...,  0.00341372,\n",
              "          0.02885464,  0.01389387],\n",
              "        ...,\n",
              "        [ 0.1128564 , -0.19083163,  0.14494908, ..., -0.24827825,\n",
              "         -0.14426842, -0.11296264],\n",
              "        [ 0.12261419, -0.00303812, -0.12378974, ...,  0.24440566,\n",
              "         -0.1132943 ,  0.15731792],\n",
              "        [-0.06008186, -0.18666956,  0.08784254, ..., -0.0380415 ,\n",
              "          0.01906254, -0.1402525 ]])}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "import tempfile\n",
        "# Instead of importing gaussian_random_matrix, import GaussianRandomProjection\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "import numpy as np # Import numpy for array operations\n",
        "\n",
        "output_dir = tempfile.mkdtemp()\n",
        "original_dim = hub.load(model_url)(['']).shape[1]\n",
        "random_projection_matrix = None\n",
        "\n",
        "if projected_dim:\n",
        "  # Create a GaussianRandomProjection object\n",
        "  transformer = GaussianRandomProjection(n_components=projected_dim)\n",
        "  # Fit the transformer to any data with the correct number of features (original_dim)\n",
        "  # Here we use a dummy array for fitting\n",
        "  transformer.fit(np.zeros((1, original_dim)))\n",
        "  # Get the projection matrix\n",
        "  random_projection_matrix = transformer.components_.T\n",
        "  print(\"A Gaussian random weight matrix was created with shape of {}\".format(random_projection_matrix.shape))\n",
        "\n",
        "\n",
        "args = {\n",
        "    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
        "    'runner': 'DirectRunner',\n",
        "    'batch_size': 1024,\n",
        "    'data_dir': 'corpus/*.txt',\n",
        "    'output_dir': output_dir,\n",
        "    'model_url': model_url,\n",
        "    'random_projection_matrix': random_projection_matrix,\n",
        "}\n",
        "\n",
        "print(\"Pipeline args are set.\")\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iS9obmeP4ZOA",
        "outputId": "62a8f697-c55a-4c08-c103-7f2215c63353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-d0d41f3b-cc48-4aec-b95d-2dfb67308ee7.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'batch_size': 1024, 'data_dir': 'corpus/*.txt', 'output_dir': '/tmp/tmpex0u2i5l', 'model_url': 'https://tfhub.dev/google/nnlm-en-dim128/2', 'random_projection_matrix': array([[ 0.0018381 , -0.03032105, -0.18692303, ..., -0.02025734,\n",
            "        -0.12891474, -0.0048825 ],\n",
            "       [-0.14750635, -0.09745011, -0.03123811, ..., -0.18726707,\n",
            "        -0.05538086,  0.0398117 ],\n",
            "       [ 0.01147445, -0.12545985,  0.13579665, ...,  0.00341372,\n",
            "         0.02885464,  0.01389387],\n",
            "       ...,\n",
            "       [ 0.1128564 , -0.19083163,  0.14494908, ..., -0.24827825,\n",
            "        -0.14426842, -0.11296264],\n",
            "       [ 0.12261419, -0.00303812, -0.12378974, ...,  0.24440566,\n",
            "        -0.1132943 ,  0.15731792],\n",
            "       [-0.06008186, -0.18666956,  0.08784254, ..., -0.0380415 ,\n",
            "         0.01906254, -0.1402525 ]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pipeline...\n",
            "CPU times: user 2.43 s, sys: 1.44 s, total: 3.87 s\n",
            "Wall time: 2.89 s\n",
            "Pipeline is done.\n"
          ]
        }
      ],
      "source": [
        "print(\"Running pipeline...\")\n",
        "%time run_hub2emb(args)\n",
        "print(\"Pipeline is done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "JAwOo7gQWvVd",
        "outputId": "3603e4a0-fe58-4d05-87c1-1d38a19516bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb-00000-of-00001.tfrecords\n"
          ]
        }
      ],
      "source": [
        "!ls {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVnee4e6U90u"
      },
      "source": [
        "생성된 임베딩의 일부를 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-K7pGXlXOj1N",
        "outputId": "5b9f3e69-986b-42c0-c1eb-12616adc9960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UpdatedTitle: [-0.09777419  0.16038282  0.07757288  0.23370871  0.07652647 -0.12696837\n",
            " -0.07989868 -0.14378484 -0.17065716  0.13668518]\n",
            "수리요청바랍니다: [ 0.01736344 -0.1256771  -0.02362035 -0.00696415 -0.08670127 -0.10139033\n",
            "  0.15703374  0.00801026 -0.0426084   0.10337427]\n",
            "안녕하세요 여기는. 오정동 우체국 옆 상가 주차장 입: [-0.0396103   0.26016995  0.1283808   0.00653004  0.05674693  0.16520491\n",
            " -0.05677558 -0.2530557   0.07675996  0.03215756]\n",
            "쓰레기기를  낮애버려놓아 더니기불편해요* 안전신문고: [-0.05548347  0.07151826  0.11606419  0.20420715 -0.104642   -0.03690207\n",
            " -0.29866475 -0.09837242 -0.02509502 -0.05896591]\n",
            "표디판이 기울어져 있어요바르게 다시 설치요청* 안: [ 0.05085089 -0.1527188   0.26022303  0.20919639 -0.20689072  0.1810183\n",
            "  0.05357693 -0.26560214  0.1782473  -0.15379234]\n"
          ]
        }
      ],
      "source": [
        "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\n",
        "sample = 5\n",
        "\n",
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'text': tf.io.FixedLenFeature([], tf.string),\n",
        "    'embedding': tf.io.FixedLenFeature([projected_dim], tf.float32)\n",
        "}\n",
        "\n",
        "def _parse_example(example):\n",
        "  # Parse the input `tf.Example` proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(embed_file)\n",
        "for record in dataset.take(sample).map(_parse_example):\n",
        "  print(\"{}: {}\".format(record['text'].numpy().decode('utf-8'), record['embedding'].numpy()[:10]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agGoaMSgY8wN"
      },
      "source": [
        "## 3. 임베딩을 위한 ANN 인덱스 빌드하기\n",
        "\n",
        "[ANNOY](https://github.com/spotify/annoy)(Approximate Nearest Neighbors Oh Yeah)는 주어진 쿼리 지점에 가까운 공간의 지점을 검색하기 위한 Python 바인딩이 있는 C++ 라이브러리입니다. 또한 메모리에 매핑되는 대규모 읽기 전용 파일 기반 데이터 구조를 생성합니다. 이는 음악 추천을 위해 [Spotify](https://www.spotify.com)에서 구축하고 사용합니다. 관심이 있으면 [NGT](https://github.com/yahoojapan/NGT), [FAISS](https://github.com/facebookresearch/faiss) 등과 같은 ANNOY의 다른 대안도 시도해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "UcPDspU3WjgH"
      },
      "outputs": [],
      "source": [
        "def build_index(embedding_files_pattern, index_filename, vector_length,\n",
        "    metric='angular', num_trees=100):\n",
        "  '''Builds an ANNOY index'''\n",
        "\n",
        "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n",
        "  # Mapping between the item and its identifier in the index\n",
        "  mapping = {}\n",
        "\n",
        "  embed_files = tf.io.gfile.glob(embedding_files_pattern)\n",
        "  num_files = len(embed_files)\n",
        "  print('Found {} embedding file(s).'.format(num_files))\n",
        "\n",
        "  item_counter = 0\n",
        "  for i, embed_file in enumerate(embed_files):\n",
        "    print('Loading embeddings in file {} of {}...'.format(i+1, num_files))\n",
        "    dataset = tf.data.TFRecordDataset(embed_file)\n",
        "    for record in dataset.map(_parse_example):\n",
        "      text = record['text'].numpy().decode(\"utf-8\")\n",
        "      embedding = record['embedding'].numpy()\n",
        "      mapping[item_counter] = text\n",
        "      annoy_index.add_item(item_counter, embedding)\n",
        "      item_counter += 1\n",
        "      if item_counter % 100000 == 0:\n",
        "        print('{} items loaded to the index'.format(item_counter))\n",
        "\n",
        "  print('A total of {} items added to the index'.format(item_counter))\n",
        "\n",
        "  print('Building the index with {} trees...'.format(num_trees))\n",
        "  annoy_index.build(n_trees=num_trees)\n",
        "  print('Index is successfully built.')\n",
        "\n",
        "  print('Saving index to disk...')\n",
        "  annoy_index.save(index_filename)\n",
        "  print('Index is saved to disk.')\n",
        "  print(\"Index file size: {} GB\".format(\n",
        "    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))\n",
        "  annoy_index.unload()\n",
        "\n",
        "  print('Saving mapping to disk...')\n",
        "  with open(index_filename + '.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  print('Mapping is saved to disk.')\n",
        "  print(\"Mapping file size: {} MB\".format(\n",
        "    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "AgyOQhUq6FNE",
        "outputId": "56412c2d-af10-4c14-cbee-44ed73990b37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 embedding file(s).\n",
            "Loading embeddings in file 1 of 1...\n",
            "A total of 10374 items added to the index\n",
            "Building the index with 100 trees...\n",
            "Index is successfully built.\n",
            "Saving index to disk...\n",
            "Index is saved to disk.\n",
            "Index file size: 0.02 GB\n",
            "Saving mapping to disk...\n",
            "Mapping is saved to disk.\n",
            "Mapping file size: 0.72 MB\n",
            "CPU times: user 7.79 s, sys: 317 ms, total: 8.11 s\n",
            "Wall time: 6.4 s\n"
          ]
        }
      ],
      "source": [
        "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
        "embedding_dimension = projected_dim\n",
        "index_filename = \"index\"\n",
        "\n",
        "!rm {index_filename}\n",
        "!rm {index_filename}.mapping\n",
        "\n",
        "%time build_index(embedding_files, index_filename, embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Ic31Tm5cgAd5",
        "outputId": "52800341-eefb-4911-9d40-5922313fd2d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus\tindex  index.ann  index.mapping  raw.tsv  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maGxDl8ufP-p"
      },
      "source": [
        "## 4. 유사성 일치에 인덱스 사용하기\n",
        "\n",
        "이제 ANN 인덱스를 사용하여 의미상 입력 쿼리에 가까운 뉴스 헤드라인을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dIs8W78fYPp"
      },
      "source": [
        "### 인덱스 및 매핑 파일 로드하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "jlTTrbQHayvb",
        "outputId": "70b11c4b-3e11-43cd-f008-e9858a323158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annoy index is loaded.\n",
            "Mapping file is loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-118-4bcb1204a779>:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  index = annoy.AnnoyIndex(embedding_dimension)\n"
          ]
        }
      ],
      "source": [
        "index = annoy.AnnoyIndex(embedding_dimension)\n",
        "index.load(index_filename, prefault=True)\n",
        "print('Annoy index is loaded.')\n",
        "with open(index_filename + '.mapping', 'rb') as handle:\n",
        "  mapping = pickle.load(handle)\n",
        "print('Mapping file is loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6liFMSUh08J"
      },
      "source": [
        "### 유사성 일치 메서드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "mUxjTag8hc16"
      },
      "outputs": [],
      "source": [
        "def find_similar_items(embedding, num_matches=5):\n",
        "  '''Finds similar items to a given embedding in the ANN index'''\n",
        "  ids = index.get_nns_by_vector(\n",
        "  embedding, num_matches, search_k=-1, include_distances=False)\n",
        "  items = [mapping[i] for i in ids]\n",
        "  return items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjerNpmZja0A"
      },
      "source": [
        "### 주어진 쿼리에서 임베딩 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "a0IIXzfBjZ19",
        "outputId": "8dc7d9df-f56b-4ef9-fffe-52cb7bde6480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the TF-Hub model...\n",
            "CPU times: user 624 ms, sys: 679 ms, total: 1.3 s\n",
            "Wall time: 1.31 s\n",
            "TF-Hub model is loaded.\n"
          ]
        }
      ],
      "source": [
        "# Load the TF-Hub model\n",
        "print(\"Loading the TF-Hub model...\")\n",
        "%time embed_fn = hub.load(model_url)\n",
        "print(\"TF-Hub model is loaded.\")\n",
        "\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "  print(\"Loading random projection matrix...\")\n",
        "  with open('random_projection_matrix', 'rb') as handle:\n",
        "    random_projection_matrix = pickle.load(handle)\n",
        "  print('random projection matrix is loaded.')\n",
        "\n",
        "def extract_embeddings(query):\n",
        "  '''Generates the embedding for the query'''\n",
        "  query_embedding =  embed_fn([query])[0].numpy()\n",
        "  if random_projection_matrix is not None:\n",
        "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
        "  return query_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "kCoCNROujEIO",
        "outputId": "873d3534-fa83-4653-92ea-ee365c1cee67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.13498034, -0.1588092 ,  0.10572661, -0.00282711,  0.02178992,\n",
              "       -0.01289755, -0.04587182,  0.10151501, -0.02941302, -0.14763536],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "extract_embeddings(\"Hello Machine Learning!\")[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def find_similar_items(embedding, num_matches=5):\n",
        "    '''Finds similar items to a given embedding in the ANN index'''\n",
        "    # Check if the embedding dimension matches the index's expected dimension\n",
        "    if embedding.shape[0] != index.d:\n",
        "        # If not, try to project the embedding to the correct dimension\n",
        "        if random_projection_matrix is not None and embedding.shape[0] == random_projection_matrix.shape[0]:\n",
        "            embedding = embedding.dot(random_projection_matrix)\n",
        "        else:\n",
        "            raise ValueError(f\"Embedding dimension ({embedding.shape[0]}) does not match index dimension ({index.d}).\"\n",
        "                             f\" Please ensure that the embedding model and random projection matrix are consistent with the index.\")\n",
        "\n",
        "    ids = index.get_nns_by_vector(\n",
        "        embedding, num_matches, search_k=-1, include_distances=False)\n",
        "    items = [mapping[i] for i in ids]\n",
        "    return items"
      ],
      "metadata": {
        "id": "KgUzavcVfr6F"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TF-Hub model\n",
        "print(\"Loading the TF-Hub model...\")\n",
        "# %time embed_fn = hub.load(model_url) # Assumed to be loaded elsewhere\n",
        "print(\"TF-Hub model is loaded.\")\n",
        "\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "  print(\"Loading random projection matrix...\")\n",
        "  with open('random_projection_matrix', 'rb') as handle:\n",
        "    random_projection_matrix = pickle.load(handle)\n",
        "  print('random projection matrix is loaded.')\n",
        "\n",
        "def extract_embeddings(query):\n",
        "  '''Generates the embedding for the query'''\n",
        "  global random_projection_matrix # Declare random_projection_matrix as global\n",
        "  query_embedding =  embed_fn([query])[0].numpy()\n",
        "  if random_projection_matrix is not None:\n",
        "    # Check if the shapes are compatible for dot product\n",
        "    if query_embedding.shape[0] != random_projection_matrix.shape[0]:\n",
        "      # If the shapes are not compatible, and random_projection_matrix is\n",
        "      # intended for dimensionality reduction, then likely\n",
        "      # random_projection_matrix needs to have dimensions (128, X)\n",
        "      # where X is the target dimensionality.\n",
        "      # Instead of transposing, reshape or recreate random_projection_matrix:\n",
        "      random_projection_matrix = np.random.rand(query_embedding.shape[0], 512)\n",
        "      # Replace 512 with your desired target dimensionality if different.\n",
        "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
        "  return query_embedding"
      ],
      "metadata": {
        "id": "dWGk-y3BispT",
        "outputId": "786e4109-6c4c-4ae4-e780-5532ab7c524e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the TF-Hub model...\n",
            "TF-Hub model is loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from annoy import AnnoyIndex # Importing AnnoyIndex\n",
        "\n",
        "def find_similar_items(embedding, num_matches=5):\n",
        "    '''Finds similar items to a given embedding in the ANN index'''\n",
        "    # Check if the embedding dimension matches the index's expected dimension\n",
        "    # Use index.f to get the dimension instead of index.d\n",
        "    if embedding.shape[0] != index.f:\n",
        "        # If not, try to project the embedding to the correct dimension\n",
        "        if random_projection_matrix is not None and embedding.shape[0] == random_projection_matrix.shape[0]:\n",
        "            embedding = embedding.dot(random_projection_matrix)\n",
        "        else:\n",
        "            raise ValueError(f\"Embedding dimension ({embedding.shape[0]}) does not match index dimension ({index.f}).\"\n",
        "                             f\" Please ensure that the embedding model and random projection matrix are consistent with the index.\")\n",
        "\n",
        "    ids = index.get_nns_by_vector(\n",
        "        embedding, num_matches, search_k=-1, include_distances=False)\n",
        "    items = [mapping[i] for i in ids]\n",
        "    return items"
      ],
      "metadata": {
        "id": "mvtiuWYwjyMg"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koINo8Du--8C"
      },
      "source": [
        "### 가장 유사한 항목을 찾기 위한 쿼리 입력하기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-35-7cbfba78879e\n",
        "\n",
        "#@title { run: \"auto\" }\n",
        "query = \"무단방치차량 치워주세요\" #@param {type:\"string\"}\n",
        "\n",
        "# Assuming these variables are defined or initialized elsewhere in your code\n",
        "# REPLACE ... WITH A VALID ANN INDEX OBJECT\n",
        "# For example, if you're using Annoy:\n",
        "!pip install annoy\n",
        "from annoy import AnnoyIndex\n",
        "# Assume embedding_dimension is defined\n",
        "embedding_dimension = 128  # Replace with your actual embedding dimension\n",
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "index.load('index.ann') # Load a pre-built index - ensure the file exists\n",
        "\n",
        "# REPLACE ... WITH THE ACTUAL MAPPING\n",
        "#mapping = {}\n",
        "\n",
        "# REPLACE ... WITH THE ACTUAL RANDOM PROJECTION MATRIX (IF USED)\n",
        "import numpy as np\n",
        "random_projection_matrix = None\n",
        "if os.path.exists('random_projection_matrix'):\n",
        "    with open('index.mapping', 'rb') as handle:\n",
        "        random_projection_matrix = pickle.load(handle)\n",
        "    print(\"Random projection matrix loaded successfully.\")\n",
        "\n",
        "print(\"Generating embedding for the query...\")\n",
        "%time query_embedding = extract_embeddings(query)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Finding relevant items in the index...\")\n",
        "%time items = find_similar_items(query_embedding, 10)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Results:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "FFiR9ggvf8pb",
        "outputId": "9eeedae0-4b94-4371-aaa6-3a8c169fdd69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annoy in /usr/local/lib/python3.10/dist-packages (1.17.3)\n",
            "Generating embedding for the query...\n",
            "CPU times: user 3.47 ms, sys: 0 ns, total: 3.47 ms\n",
            "Wall time: 2.93 ms\n",
            "\n",
            "Finding relevant items in the index...\n",
            "CPU times: user 1.28 ms, sys: 784 µs, total: 2.07 ms\n",
            "Wall time: 1.56 ms\n",
            "\n",
            "Results:\n",
            "=========\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n",
            "Unknown item\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from annoy import AnnoyIndex\n",
        "import pickle\n",
        "\n",
        "# 1. TF-Hub 모델 로드\n",
        "model_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"  # 사용할 TF-Hub 모델 URL (128 차원 임베딩)\n",
        "embed_fn = hub.load(model_url)\n",
        "\n",
        "# 2. 파일 로드 및 데이터 준비\n",
        "file_path = '/content/sample_data/민원분류_title.csv'\n",
        "data_df = pd.read_csv(file_path)\n",
        "data = data_df['UpdatedTitle'].dropna().tolist()  # 필요한 열로 수정\n",
        "\n",
        "# 설정\n",
        "embedding_dimension = 128  # TF-Hub 모델에 따른 임베딩 차원\n",
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "\n",
        "# 매핑 딕셔너리 초기화\n",
        "mapping = {}\n",
        "\n",
        "# 3. 임베딩 생성 함수\n",
        "def extract_embeddings(text):\n",
        "    embedding = embed_fn([text])[0].numpy()  # TF-Hub 모델로 임베딩 생성\n",
        "    return embedding\n",
        "\n",
        "# 4. 인덱스에 데이터 추가\n",
        "for i, text in enumerate(data):\n",
        "    embedding = extract_embeddings(text)\n",
        "    index.add_item(i, embedding)  # Annoy 인덱스에 임베딩 추가\n",
        "    mapping[i] = text  # 매핑에 텍스트 저장\n",
        "\n",
        "# 5. 인덱스 빌드 및 저장\n",
        "index.build(10)  # 트리 개수 조정 가능\n",
        "index.save('index.ann')\n",
        "\n",
        "# 6. 매핑 파일 저장\n",
        "with open('index.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"ANN index and mapping files saved successfully.\")\n",
        "\n",
        "# 7. 유사 항목 검색 함수\n",
        "def find_similar_items(query_embedding, num_matches=5):\n",
        "    item_ids = index.get_nns_by_vector(query_embedding, num_matches)\n",
        "    items = [mapping.get(item_id, \"Unknown item\") for item_id in item_ids]\n",
        "    return items\n",
        "\n",
        "# 8. 쿼리 입력 및 임베딩 생성 후 검색\n",
        "query = \"버스차로 위반\"  # 예시 질의어\n",
        "query_embedding = extract_embeddings(query)  # 쿼리에 대한 임베딩 생성\n",
        "\n",
        "# 검색 결과 출력\n",
        "items = find_similar_items(query_embedding, 5)\n",
        "\n",
        "print(\"\\nResults for query '{}':\".format(query))\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "xPDe94KnA1Ej",
        "outputId": "3c75b484-1e08-402d-c03d-435187e23f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN index and mapping files saved successfully.\n",
            "\n",
            "Results for query '버스차로 위반':\n",
            "=========\n",
            " 자전거보관소 직사각형 개선 하세요\n",
            "금연구역  개선 하세요\n",
            "아파트 헬스장 관련 문의\n",
            "기타 불법 주정차 신고입니다. 코너 꺾는곳에다가 차를 \n",
            "아파트 입구 주차봉 설치 관련\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkRSqs77tDuX"
      },
      "source": [
        "## 더 자세히 알고 싶나요?\n",
        "\n",
        "[tensorflow.org](https://www.tensorflow.org/)에서 TensorFlow에 대해 자세히 알아보고 [tensorflow.org/hub](https://www.tensorflow.org/hub/)에서 TF-Hub API 설명서를 확인할 수 있습니다. 추가적인 텍스트 임베딩 모듈 및 이미지 특성 벡터 모듈을 포함해 [tfhub.dev](https://tfhub.dev/)에서 사용 가능한 TensorFlow Hub 모델을 찾아보세요.\n",
        "\n",
        "빠르게 진행되는 Google의 머신러닝 실무 개요 과정인 [머신러닝 집중 과정](https://developers.google.com/machine-learning/crash-course/)도 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "# 인덱스 로드\n",
        "embedding_dimension = 512  # 실제 임베딩 차원에 맞게 설정\n",
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "index.load('index.ann')  # 저장된 인덱스 파일 로드\n",
        "\n",
        "# `index.mapping` 파일을 pickle로 로드\n",
        "with open('index.mapping', 'rb') as handle:\n",
        "    mapping = pickle.load(handle)\n",
        "print(\"Mapping loaded successfully.\")\n",
        "\n",
        "# 유사 항목 검색 함수\n",
        "def find_similar_items(embedding, num_matches=5):\n",
        "    ids = index.get_nns_by_vector(embedding, num_matches, search_k=-1, include_distances=False)\n",
        "    items = [mapping.get(i, \"Unknown item\") for i in ids]\n",
        "    return items\n",
        "\n",
        "# 예시 쿼리\n",
        "query = \"쓰레기 무단투기\"\n",
        "query_embedding = extract_embeddings(query)  # 쿼리에 대한 임베딩 생성\n",
        "\n",
        "# 검색 결과 출력\n",
        "items = find_similar_items(query_embedding, 10)\n",
        "print(\"\\nResults:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "VX9tq31l8BQH",
        "outputId": "cf6acfda-e9e4-4c48-fbe9-1ab8d9d2c61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping loaded successfully.\n",
            "\n",
            "Results:\n",
            "=========\n",
            "수리요청바랍니다\n",
            "UpdatedTitle\n",
            "안녕하세요 여기는. 오정동 우체국 옆 상가 주차장 입\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from annoy import AnnoyIndex\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 예시 데이터 및 인덱스 설정\n",
        "embedding_dimension = 512\n",
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "\n",
        "# 새로 생성한 `mapping`을 저장하는 코드\n",
        "mapping = {0: \"Example item 1\", 1: \"Example item 2\", 2: \"Example item 3\"}  # 실제 데이터로 대체 필요\n",
        "\n",
        "# Annoy 인덱스 빌드 및 저장 (임베딩은 예제용, 실제 임베딩으로 대체)\n",
        "index.add_item(0, np.random.rand(embedding_dimension))\n",
        "index.add_item(1, np.random.rand(embedding_dimension))\n",
        "index.add_item(2, np.random.rand(embedding_dimension))\n",
        "index.build(10)\n",
        "index.save('index.ann')\n",
        "\n",
        "# 새로 생성한 mapping 저장\n",
        "with open('index.mapping', 'wb') as handle:\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(\"Mapping file saved successfully.\")"
      ],
      "metadata": {
        "id": "3db-T4Hb8xE_",
        "outputId": "efc4133d-d2a9-4d08-b281-afd308825d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `index.mapping` 파일을 pickle로 로드\n",
        "with open('index.mapping', 'rb') as handle:\n",
        "    loaded_mapping = pickle.load(handle)\n",
        "print(\"Loaded mapping:\", loaded_mapping)"
      ],
      "metadata": {
        "id": "aHi8_I3A9jZ1",
        "outputId": "6f07e1f7-0102-46d2-bdb4-de11122c62e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded mapping: {0: 'Example item 1', 1: 'Example item 2', 2: 'Example item 3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex\n",
        "import numpy as np\n",
        "\n",
        "# 설정\n",
        "embedding_dimension = 512\n",
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "\n",
        "# 예시 데이터 추가 및 인덱스 빌드\n",
        "for i in range(1000):  # 예시 데이터의 수에 맞게 반복\n",
        "    vector = np.random.rand(embedding_dimension)\n",
        "    index.add_item(i, vector)\n",
        "\n",
        "index.build(1000)  # 인덱스 빌드\n",
        "index.save('index.ann')\n",
        "\n",
        "# 매핑 데이터 확인\n",
        "mapping = {i: f\"Example item {i}\" for i in range(10)}\n",
        "\n",
        "# 예시 embedding 생성 함수\n",
        "def extract_embeddings(text):\n",
        "    return np.random.rand(embedding_dimension)\n",
        "\n",
        "# 예시 유사 항목 검색 함수\n",
        "def find_similar_items(query_embedding, num_items=5):\n",
        "    item_ids = index.get_nns_by_vector(query_embedding, num_items)\n",
        "    items = [mapping.get(item_id, \"Unknown item\") for item_id in item_ids]\n",
        "    return items\n",
        "\n",
        "# 검색 쿼리 임베딩 생성 및 유사 항목 찾기\n",
        "query = \"쓰레기무단투기\"\n",
        "query_embedding = extract_embeddings(query)\n",
        "items = find_similar_items(query_embedding, 5)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\nResults:\")\n",
        "print(\"=========\")\n",
        "for item in items:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "cI_krR7ho6vo",
        "outputId": "3924d4b2-936f-40c8-abdd-df3c9b95a9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "=========\n",
            "Example item 3\n",
            "Example item 6\n",
            "Example item 8\n",
            "Example item 5\n",
            "Example item 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = AnnoyIndex(embedding_dimension, 'angular')\n",
        "index.load('index.ann')  # 저장한 파일 로드"
      ],
      "metadata": {
        "id": "veS-VZU_1PJx",
        "outputId": "c1760a64-7beb-488e-a327-d43f1d898cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ACbjNjyO4f_8",
        "g6pXBVxsVUbm"
      ],
      "name": "tf2_semantic_approximate_nearest_neighbors.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}