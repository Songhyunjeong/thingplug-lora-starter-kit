{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Songhyunjeong/thingplug-lora-starter-kit/blob/master/crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "z2-319JPZcCS",
        "outputId": "5306ca87-6544-4163-be3e-cfad19fb5bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/chrome-for-testing-public/129.0.6668.58/linux64/chromedriver-linux64.zip\n",
        "!unzip /content/chromedriver-linux64.zip\n",
        "!chmod +x '/content/chromedriver-linux64/chromedriver'\n",
        "!mv '/content/chromedriver-linux64/chromedriver' /usr/local/bin/"
      ],
      "metadata": {
        "id": "Z4_eqg5IGI7P",
        "outputId": "3700f8c2-2bbd-441f-ebff-4818a1ce2fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 11:23:21--  https://storage.googleapis.com/chrome-for-testing-public/129.0.6668.58/linux64/chromedriver-linux64.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.207, 142.250.157.207, 142.251.8.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9410841 (9.0M) [application/zip]\n",
            "Saving to: ‘chromedriver-linux64.zip’\n",
            "\n",
            "chromedriver-linux6 100%[===================>]   8.97M  6.45MB/s    in 1.4s    \n",
            "\n",
            "2024-10-01 11:23:23 (6.45 MB/s) - ‘chromedriver-linux64.zip’ saved [9410841/9410841]\n",
            "\n",
            "Archive:  /content/chromedriver-linux64.zip\n",
            "  inflating: chromedriver-linux64/LICENSE.chromedriver  \n",
            "  inflating: chromedriver-linux64/THIRD_PARTY_NOTICES.chromedriver  \n",
            "  inflating: chromedriver-linux64/chromedriver  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y"
      ],
      "metadata": {
        "id": "4HdnFn3EPHDr",
        "outputId": "1c29b3d9-6738-41a5-a4b3-ba172b50529c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 11:23:31--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.204.93, 74.125.204.136, 74.125.204.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.204.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111860972 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 106.68M   292MB/s    in 0.4s    \n",
            "\n",
            "2024-10-01 11:23:32 (292 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [111860972/111860972]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123614 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (129.0.6668.70-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.2 [10.7 MB]\n",
            "Fetched 10.9 MB in 3s (3,388 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 123731 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up google-chrome-stable (129.0.6668.70-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chromium-browser --version\n",
        "!chromedriver --version"
      ],
      "metadata": {
        "id": "_I5OFXLuF6PO",
        "outputId": "16f905e3-6b2d-4b86-c757-5d66d93595ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: chromium-browser: command not found\n",
            "ChromeDriver 129.0.6668.58 (81a06fb873a9b386848719cf9f93e59579fb5d4b-refs/branch-heads/6668@{#1318})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!google-chrome --version"
      ],
      "metadata": {
        "id": "ui3bhqAiILhd",
        "outputId": "6c41aeb3-70ca-46f4-a6ff-7d08e116f753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Chrome 129.0.6668.70 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Chrome 옵션 설정\n",
        "chrome_options = Options()\n",
        "chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome의 경로 명시\n",
        "chrome_options.add_argument('--headless')  # 브라우저를 백그라운드에서 실행\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--disable-gpu')\n",
        "chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "\n",
        "# Chromedriver 경로 설정\n",
        "service = Service('/usr/local/bin/chromedriver')\n",
        "\n",
        "# WebDriver 실행\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Google 페이지 열기\n",
        "driver.get(\"https://www.naver.com\")\n",
        "\n",
        "# 페이지 제목 출력\n",
        "print(driver.title)\n",
        "\n",
        "# WebDriver 종료\n",
        "#driver.quit()"
      ],
      "metadata": {
        "id": "lyCf9EAF5uM8",
        "outputId": "9e5ebe9e-5e1c-4cd0-a2d9-f3d3f634d3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAVER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.common.exceptions import WebDriverException\n",
        "import time\n",
        "from urllib.parse import urlencode, quote\n",
        "# Your Naver credentials\n",
        "naver_id = \"postpia\"         # Replace with your Naver ID\n",
        "naver_password = \"na700925!shj\"   # Replace with your Naver password\n",
        "\n",
        "# URL for Naver login\n",
        "login_url = \"https://nid.naver.com/nidlogin.login\"\n",
        "\n",
        "\n",
        "# Initialize the Selenium driver function to allow restarting\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome의 경로 명시\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line to disable headless mode for troubleshooting\n",
        "\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to login to Naver\n",
        "def login_to_naver(driver, naver_id, naver_password):\n",
        "    driver.get(login_url)\n",
        "    time.sleep(2)  # Wait for the page to load\n",
        "\n",
        "    # Enter the ID\n",
        "    id_field = driver.find_element(By.ID, \"id\")\n",
        "    id_field.send_keys(naver_id)\n",
        "\n",
        "    # Enter the Password\n",
        "    password_field = driver.find_element(By.ID, \"pw\")\n",
        "    password_field.send_keys(naver_password)\n",
        "\n",
        "    # Click the login button\n",
        "    login_button = driver.find_element(By.CLASS_NAME, \"btn_login\")\n",
        "    login_button.click()\n",
        "\n",
        "    time.sleep(5)  # Wait for the login to complete\n",
        "\n",
        "# Start the driver and log in\n",
        "driver = initialize_driver()\n",
        "login_to_naver(driver, naver_id, naver_password)\n",
        "\n",
        "\n",
        "# Start from a given article ID\n",
        "start_article_id = 72546840  # Replace with your starting article ID\n",
        "\n",
        "# Base URL structure\n",
        "#base_url = \"https://cafe.naver.com/ArticleRead.nhn?clubid=10094499&boardtype=L&articleid=\"\n",
        "base_url = \"%2FArticleRead.nhn%253Fclubid%3D10094499%2526page%3D1%2526boardtype%3DL%2526articleid%3D\"\n",
        "#https://cafe.naver.com/imsanbu?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D10094499%2526page%3D1%2526boardtype%3DL%2526articleid%3D72547237%2526referrerAllArticles%3Dtrue\n",
        "\n",
        "#decoded_url = \"/ArticleRead.nhn?clubid=10094499&page=1&boardtype=L&articleid=72547237&referrerAllArticles=true\"\n",
        "\n",
        "# Encode the URL back to match the original format\n",
        "#encoded_url = quote(decoded_url, safe='')\n",
        "\n",
        "# Construct the full URL in the required format\n",
        "#article_url = f\"https://cafe.naver.com/imsanbu?iframe_url_utf8={encoded_url}\"\n",
        "\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "article_data = []\n",
        "\n",
        "# Define the number of articles to scrape\n",
        "number_of_articles = 10  # Set this value to 100\n",
        "\n",
        "# Iterate over the articles\n",
        "\n",
        "for article_id in range(start_article_id, start_article_id - number_of_articles, -1):\n",
        "    #article_url = f\"{base_url}{article_id}&referrerAllArticles=true\"\n",
        "    decoded_url = f\"{base_url}{article_id}%2526referrerAllArticles%3Dtrue\"\n",
        "    encoded_url = quote(decoded_url, safe='')\n",
        "    article_url = f\"https://cafe.naver.com/imsanbu?iframe_url_utf8={encoded_url}\"\n",
        "    try:\n",
        "        driver.get(article_url)\n",
        "        # Wait for the page to load\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Check if an alert is present and handle it\n",
        "        try:\n",
        "            alert = driver.switch_to.alert\n",
        "            print(f\"Alert present for article ID {article_id}: {alert.text}\")\n",
        "            alert.accept()\n",
        "            continue  # Skip to the next article\n",
        "        except NoAlertPresentException:\n",
        "            pass  # No alert found, continue normally\n",
        "\n",
        "        # Get the page source\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        # Extract category\n",
        "        category_element = soup.select_one(\"a.link_board\")\n",
        "        category = category_element.get_text(strip=True) if category_element else \"N/A\"\n",
        "\n",
        "        # Extract title\n",
        "        title_element = soup.select_one(\"h3.title_text\")\n",
        "        title = title_element.get_text(strip=True) if title_element else \"N/A\"\n",
        "\n",
        "        # Extract author\n",
        "        author_element = soup.select_one(\"button.nickname\")\n",
        "        author = author_element.get_text(strip=True) if author_element else \"N/A\"\n",
        "\n",
        "        # Extract date\n",
        "        date_element = soup.select_one(\"span.date\")\n",
        "        date = date_element.get_text(strip=True) if date_element else \"N/A\"\n",
        "\n",
        "        # Extract content (text inside <span> tags)\n",
        "        content_elements = soup.select(\"div.se-module.se-module-text span\")\n",
        "        content = \" \".join([elem.get_text(strip=True) for elem in content_elements]) if content_elements else \"N/A\"\n",
        "\n",
        "        # Append the data to the list\n",
        "        article_data.append({\n",
        "            \"Article ID\": article_id,\n",
        "            \"Category\": category,\n",
        "            \"Title\": title,\n",
        "            \"Author\": author,\n",
        "            \"Date\": date,\n",
        "            \"Content\": content\n",
        "        })\n",
        "\n",
        "        print(f\"Scraped article ID: {article_id}\")\n",
        "\n",
        "    except UnexpectedAlertPresentException as e:\n",
        "        print(f\"Skipping article ID {article_id} due to alert: {str(e)}\")\n",
        "        driver.switch_to.alert.accept()  # Close the alert and continue\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"WebDriverException occurred for article ID {article_id}: {str(e)}\")\n",
        "        print(\"Restarting ChromeDriver...\")\n",
        "        driver.quit()\n",
        "        driver = initialize_driver()  # Restart the driver\n",
        "        continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape article ID {article_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Close the Selenium driver\n",
        "driver.quit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(article_data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv(\"naver_cafe_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Crawling completed! Data saved to 'naver_cafe_articles.csv'\")"
      ],
      "metadata": {
        "id": "VJCyt0hFQ4rQ",
        "outputId": "b3b46d94-d836-40a1-ea84-31ae21a91495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped article ID: 72546840\n",
            "Scraped article ID: 72546839\n",
            "Scraped article ID: 72546838\n",
            "Scraped article ID: 72546837\n",
            "Scraped article ID: 72546836\n",
            "Scraped article ID: 72546835\n",
            "Scraped article ID: 72546834\n",
            "Scraped article ID: 72546833\n",
            "Scraped article ID: 72546832\n",
            "Scraped article ID: 72546831\n",
            "Crawling completed! Data saved to 'naver_cafe_articles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException, UnexpectedAlertPresentException, NoAlertPresentException\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Your Naver credentials\n",
        "naver_id = \"postpia\"         # Replace with your Naver ID\n",
        "naver_password = \"na700925!shj\"  # Replace with your Naver password\n",
        "\n",
        "# URL for Naver login\n",
        "login_url = \"https://nid.naver.com/nidlogin.login\"\n",
        "\n",
        "# Initialize the Selenium driver function to allow restarting\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome의 경로 명시\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line to disable headless mode for troubleshooting\n",
        "\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to log in to Naver\n",
        "def login_to_naver(driver, naver_id, naver_password):\n",
        "    driver.get(login_url)\n",
        "    time.sleep(2)  # Wait for the page to load\n",
        "\n",
        "    # Enter the ID\n",
        "    id_field = driver.find_element(By.ID, \"id\")\n",
        "    id_field.send_keys(naver_id)\n",
        "\n",
        "    # Enter the Password\n",
        "    password_field = driver.find_element(By.ID, \"pw\")\n",
        "    password_field.send_keys(naver_password)\n",
        "\n",
        "    # Click the login button\n",
        "    login_button = driver.find_element(By.CLASS_NAME, \"btn_login\")\n",
        "    login_button.click()\n",
        "\n",
        "    time.sleep(5)  # Wait for the login to complete\n",
        "\n",
        "# Start the driver and log in\n",
        "driver = initialize_driver()\n",
        "login_to_naver(driver, naver_id, naver_password)\n",
        "# Start from a given article ID\n",
        "start_article_id = 72546840  # Replace with your starting article ID\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "article_data = []\n",
        "\n",
        "# Define the number of articles to scrape\n",
        "number_of_articles = 10  # Adjust as needed\n",
        "\n",
        "# Iterate over the articles\n",
        "for article_id in range(start_article_id, start_article_id - number_of_articles, -1):\n",
        "    article_url = f\"https://cafe.naver.com/imsanbu?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D10094499%2526page%3D1%2526boardtype%3DL%2526articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
        "\n",
        "    try:\n",
        "        driver.get(article_url)\n",
        "\n",
        "        # Wait longer for dynamic content to load\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Get the page source\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        # Debugging: Check the loaded HTML content\n",
        "        with open(f\"article_{article_id}.html\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(soup.prettify())  # Save the HTML content for manual inspection\n",
        "\n",
        "        # Extract category\n",
        "        category_element = soup.select_one(\"a.link_board\")\n",
        "        category = category_element.get_text(strip=True) if category_element else \"N/A\"\n",
        "\n",
        "        # Extract title\n",
        "        title_element = soup.select_one(\"h3.title_text\")\n",
        "        title = title_element.get_text(strip=True) if title_element else \"N/A\"\n",
        "\n",
        "        # Extract author\n",
        "        author_element = soup.select_one(\"button.nickname\")\n",
        "        author = author_element.get_text(strip=True) if author_element else \"N/A\"\n",
        "\n",
        "        # Extract date\n",
        "        date_element = soup.select_one(\"span.date\")\n",
        "        date = date_element.get_text(strip=True) if date_element else \"N/A\"\n",
        "\n",
        "        # Extract content (make sure we target the correct <span> elements)\n",
        "        content_elements = soup.select(\"div.se-module.se-module-text span\")\n",
        "        content = \" \".join([elem.get_text(strip=True) for elem in content_elements]) if content_elements else \"N/A\"\n",
        "\n",
        "        # Append the data to the list\n",
        "        article_data.append({\n",
        "            \"Article ID\": article_id,\n",
        "            \"Category\": category,\n",
        "            \"Title\": title,\n",
        "            \"Author\": author,\n",
        "            \"Date\": date,\n",
        "            \"Content\": content\n",
        "        })\n",
        "\n",
        "        print(f\"Scraped article ID: {article_id}\")\n",
        "\n",
        "    except UnexpectedAlertPresentException as e:\n",
        "        print(f\"Alert present for article ID {article_id}: {str(e)}. Skipping this article.\")\n",
        "        try:\n",
        "            alert = driver.switch_to.alert\n",
        "            alert.accept()  # Close the alert\n",
        "        except NoAlertPresentException:\n",
        "            pass  # Alert might already be closed\n",
        "        continue  # Move on to the next article\n",
        "    except WebDriverException as e:\n",
        "        print(f\"WebDriverException occurred for article ID {article_id}: {str(e)}\")\n",
        "        print(\"Restarting ChromeDriver...\")\n",
        "        driver.quit()\n",
        "        driver = initialize_driver()  # Restart the driver\n",
        "        continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape article ID {article_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Close the Selenium driver\n",
        "driver.quit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(article_data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv(\"naver_cafe_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Crawling completed! Data saved to 'naver_cafe_articles.csv'\")"
      ],
      "metadata": {
        "id": "8gaeLpf4do25",
        "outputId": "494ad33e-e68e-475c-b69f-98b6abb451f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alert present for article ID 72546840: Alert Text: 삭제되었거나 없는 게시글입니다.\n",
            "Message: unexpected alert open: {Alert text : 삭제되었거나 없는 게시글입니다.}\n",
            "  (Session info: chrome=129.0.6668.70)\n",
            "Stacktrace:\n",
            "#0 0x5b735a3d313a <unknown>\n",
            "#1 0x5b735a0b95e0 <unknown>\n",
            "#2 0x5b735a14d658 <unknown>\n",
            "#3 0x5b735a12d8c3 <unknown>\n",
            "#4 0x5b735a0fb6b3 <unknown>\n",
            "#5 0x5b735a0fc68e <unknown>\n",
            "#6 0x5b735a39db3b <unknown>\n",
            "#7 0x5b735a3a1ac1 <unknown>\n",
            "#8 0x5b735a38a335 <unknown>\n",
            "#9 0x5b735a3a2642 <unknown>\n",
            "#10 0x5b735a36f49f <unknown>\n",
            "#11 0x5b735a3c2038 <unknown>\n",
            "#12 0x5b735a3c2203 <unknown>\n",
            "#13 0x5b735a3d1f8c <unknown>\n",
            "#14 0x7c0d104d4ac3 <unknown>\n",
            ". Skipping this article.\n",
            "Scraped article ID: 72546839\n",
            "Scraped article ID: 72546838\n",
            "Scraped article ID: 72546837\n",
            "Alert present for article ID 72546836: Alert Text: 삭제되었거나 없는 게시글입니다.\n",
            "Message: unexpected alert open: {Alert text : 삭제되었거나 없는 게시글입니다.}\n",
            "  (Session info: chrome=129.0.6668.70)\n",
            "Stacktrace:\n",
            "#0 0x5b735a3d313a <unknown>\n",
            "#1 0x5b735a0b95e0 <unknown>\n",
            "#2 0x5b735a14d658 <unknown>\n",
            "#3 0x5b735a12d8c3 <unknown>\n",
            "#4 0x5b735a0fb6b3 <unknown>\n",
            "#5 0x5b735a0fc68e <unknown>\n",
            "#6 0x5b735a39db3b <unknown>\n",
            "#7 0x5b735a3a1ac1 <unknown>\n",
            "#8 0x5b735a38a335 <unknown>\n",
            "#9 0x5b735a3a2642 <unknown>\n",
            "#10 0x5b735a36f49f <unknown>\n",
            "#11 0x5b735a3c2038 <unknown>\n",
            "#12 0x5b735a3c2203 <unknown>\n",
            "#13 0x5b735a3d1f8c <unknown>\n",
            "#14 0x7c0d104d4ac3 <unknown>\n",
            ". Skipping this article.\n",
            "Scraped article ID: 72546835\n",
            "Scraped article ID: 72546834\n",
            "Scraped article ID: 72546833\n",
            "Scraped article ID: 72546832\n",
            "Scraped article ID: 72546831\n",
            "Crawling completed! Data saved to 'naver_cafe_articles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException, UnexpectedAlertPresentException, NoAlertPresentException\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Your Naver credentials\n",
        "naver_id = \"postpia\"         # Replace with your Naver ID\n",
        "naver_password = \"na700925!shj\"    # Replace with your Naver password\n",
        "\n",
        "# URL for Naver login\n",
        "login_url = \"https://nid.naver.com/nidlogin.login\"\n",
        "\n",
        "# Initialize the Selenium driver function\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line to disable headless mode if you want to see the browser\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to log in to Naver\n",
        "def login_to_naver(driver, naver_id, naver_password):\n",
        "    driver.get(login_url)\n",
        "    time.sleep(2)  # Wait for the page to load\n",
        "\n",
        "    # Enter the ID\n",
        "    id_field = driver.find_element(By.ID, \"id\")\n",
        "    id_field.send_keys(naver_id)\n",
        "\n",
        "    # Enter the Password\n",
        "    password_field = driver.find_element(By.ID, \"pw\")\n",
        "    password_field.send_keys(naver_password)\n",
        "\n",
        "    # Click the login button\n",
        "    login_button = driver.find_element(By.CLASS_NAME, \"btn_login\")\n",
        "    login_button.click()\n",
        "\n",
        "    time.sleep(5)  # Wait for the login to complete\n",
        "\n",
        "# Start the driver and log in\n",
        "driver = initialize_driver()\n",
        "login_to_naver(driver, naver_id, naver_password)\n",
        "\n",
        "# Start from a given article ID\n",
        "start_article_id = 72557150  # Replace with your starting article ID\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "article_data = []\n",
        "\n",
        "# Define the number of articles to scrape\n",
        "number_of_articles = 20  # Set this value to the number of articles you want to scrape\n",
        "\n",
        "# Iterate over the articles\n",
        "for article_id in range(start_article_id, start_article_id - number_of_articles, -1):\n",
        "    # Construct the URL in the required format\n",
        "    article_url = f\"https://cafe.naver.com/imsanbu?iframe_url_utf8=%2FArticleRead.nhn%253Fclubid%3D10094499%26search.menuid=818%26search.boardtype=L%26articleid%3D{article_id}%2526referrerAllArticles%3Dtrue\"\n",
        "\n",
        "    try:\n",
        "        driver.get(article_url)\n",
        "        # Wait for the page to load\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Switch to the iframe that contains the article content\n",
        "        try:\n",
        "            iframe = driver.find_element(By.CSS_SELECTOR, \"iframe#cafe_main\")\n",
        "            driver.switch_to.frame(iframe)\n",
        "        except Exception as iframe_error:\n",
        "            print(f\"Could not switch to iframe for article ID {article_id}: {iframe_error}\")\n",
        "            continue  # Skip to the next article\n",
        "\n",
        "        # Get the page source from within the iframe\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        category_element = soup.select_one(\"a.link_board\")\n",
        "        if category_element and category_element.get_text(strip=True).endswith('수다방'):\n",
        "            category = category_element.get_text(strip=True)\n",
        "            # Proceed with crawling as this post belongs to a category ending with '수다방'\n",
        "            print(f\"Crawling post in category: {category}\")\n",
        "        else:\n",
        "            category = category_element.get_text(strip=True) if category_element else \"N/A\"\n",
        "            print(f\"Skipping post in category: {category}\")\n",
        "            continue  # Skip this article as it's not in the desired category\n",
        "\n",
        "        # Extract title\n",
        "        title_element = soup.select_one(\"h3.title_text\")\n",
        "        title = title_element.get_text(strip=True) if title_element else \"N/A\"\n",
        "\n",
        "        # Extract author\n",
        "        author_element = soup.select_one(\"button.nickname\")\n",
        "        author = author_element.get_text(strip=True) if author_element else \"N/A\"\n",
        "\n",
        "        # Extract date\n",
        "        date_element = soup.select_one(\"span.date\")\n",
        "        date = date_element.get_text(strip=True) if date_element else \"N/A\"\n",
        "\n",
        "        # Extract content (make sure to target the correct <span> elements)\n",
        "        content_elements = soup.select(\"div.se-module.se-module-text span\")\n",
        "        content = \" \".join([elem.get_text(strip=True) for elem in content_elements]) if content_elements else \"N/A\"\n",
        "\n",
        "        # Append the data to the list\n",
        "        article_data.append({\n",
        "            \"Article ID\": article_id,\n",
        "            \"Category\": category,\n",
        "            \"Title\": title,\n",
        "            \"Author\": author,\n",
        "            \"Date\": date,\n",
        "            \"Content\": content\n",
        "        })\n",
        "\n",
        "        print(f\"Scraped article ID: {article_id}\")\n",
        "\n",
        "    except UnexpectedAlertPresentException as e:\n",
        "        print(f\"Alert present for article ID {article_id}: {str(e)}. Skipping this article.\")\n",
        "        try:\n",
        "            alert = driver.switch_to.alert\n",
        "            alert.accept()  # Close the alert\n",
        "        except NoAlertPresentException:\n",
        "            pass  # Alert might already be closed\n",
        "        continue  # Move on to the next article\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"WebDriverException occurred for article ID {article_id}: {str(e)}\")\n",
        "        driver.quit()\n",
        "        driver = initialize_driver()  # Restart the driver\n",
        "        login_to_naver(driver, naver_id, naver_password)  # Log in again\n",
        "        continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape article ID {article_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Always switch back to the main content after processing an article\n",
        "    driver.switch_to.default_content()\n",
        "\n",
        "# Close the Selenium driver\n",
        "driver.quit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(article_data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv(\"naver_cafe_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Crawling completed! Data saved to 'naver_cafe_articles.csv'\")"
      ],
      "metadata": {
        "id": "6yPL5xGp-2ha",
        "outputId": "d91cc4d9-4b5a-461d-f67e-c5cd4335b912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post in category: 육아 질문방\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: 쇼핑할인 정보방\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: N/A\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Skipping post in category: 돌/백일/산후조리 공구-매주(화)\n",
            "Could not switch to iframe for article ID 72557131: Alert Text: 삭제되었거나 없는 게시글입니다.\n",
            "Message: unexpected alert open: {Alert text : 삭제되었거나 없는 게시글입니다.}\n",
            "  (Session info: chrome=129.0.6668.70)\n",
            "Stacktrace:\n",
            "#0 0x599f0ab2413a <unknown>\n",
            "#1 0x599f0a80a5e0 <unknown>\n",
            "#2 0x599f0a89e658 <unknown>\n",
            "#3 0x599f0a87e8c3 <unknown>\n",
            "#4 0x599f0a84c6b3 <unknown>\n",
            "#5 0x599f0a84d68e <unknown>\n",
            "#6 0x599f0aaeeb3b <unknown>\n",
            "#7 0x599f0aaf2ac1 <unknown>\n",
            "#8 0x599f0aadb335 <unknown>\n",
            "#9 0x599f0aaf3642 <unknown>\n",
            "#10 0x599f0aac049f <unknown>\n",
            "#11 0x599f0ab13038 <unknown>\n",
            "#12 0x599f0ab13203 <unknown>\n",
            "#13 0x599f0ab22f8c <unknown>\n",
            "#14 0x7d1801601ac3 <unknown>\n",
            "\n",
            "Crawling completed! Data saved to 'naver_cafe_articles.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchFrameException, NoSuchElementException\n",
        "\n",
        "# Initialize the Selenium driver function\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome의 경로 명시\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line if you want to see the browser\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to switch to frame 'cafe_main' with retries\n",
        "def switch_to_cafe_frame():\n",
        "    try:\n",
        "        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, \"cafe_main\")))\n",
        "        print(\"Successfully switched to the 'cafe_main' frame\")\n",
        "    except (TimeoutException, NoSuchFrameException):\n",
        "        print(\"The 'cafe_main' frame could not be found or did not load in time.\")\n",
        "        driver.quit()\n",
        "        exit()\n",
        "\n",
        "# Start the driver and log in\n",
        "driver = initialize_driver()\n",
        "\n",
        "# Log in to Naver\n",
        "driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
        "time.sleep(2)\n",
        "driver.find_element(By.ID, \"id\").send_keys(\"postpia\")  # Replace with your Naver ID\n",
        "driver.find_element(By.ID, \"pw\").send_keys(\"na700925!shj\")  # Replace with your Naver password\n",
        "driver.find_element(By.CLASS_NAME, \"btn_login\").click()\n",
        "time.sleep(5)  # Wait for login to complete\n",
        "\n",
        "# Navigate to the cafe URL and search for '행복'\n",
        "driver.get(\"https://cafe.naver.com/imsanbu?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10094499%26search.menuid=179%26search.boardtype=L\")\n",
        "#driver.get(\"https://cafe.naver.com/mapomommy?iframe_url=%2FArticleRead.nhn%3Fclubid%3D18087572%26articleid%3D1316336%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMzE2MzM2LCJjYWZlVXJsIjoibWFwb21vbW15IiwiaXNzdWVkQXQiOjE3Mjc0Mjc5MzI4MTJ9.Jc043aCLCwMc_AyzlXc2Akg9oSYjU5vEF-ebjev_Epo%26query%3D%25EC%259C%25A1%25EC%2595%2584\")\n",
        "driver.implicitly_wait(3)\n",
        "driver.find_element(By.NAME, 'query').send_keys('행복')\n",
        "driver.find_element(By.NAME, 'query').send_keys(Keys.ENTER)\n",
        "time.sleep(2)\n",
        "\n",
        "# Switch to the frame\n",
        "switch_to_cafe_frame()\n",
        "\n",
        "# Initialize lists to store titles and content\n",
        "all_titles = []\n",
        "all_contents = []\n",
        "\n",
        "# Loop through pages\n",
        "# Loop through pages\n",
        "for i in range(1, 3):\n",
        "    req = driver.page_source\n",
        "    soup = BeautifulSoup(req, 'html.parser')\n",
        "\n",
        "    # Corrected selector to get all rows in the table\n",
        "    titles = soup.select(\"#main-area > div > table > tbody > tr\")\n",
        "\n",
        "    print(f'---- Page {i} ----')\n",
        "\n",
        "    for index, title in enumerate(titles, start=1):\n",
        "        article_link = title.select_one('td.td_article > div.board-list > div > a')\n",
        "        if article_link:\n",
        "            article_title = ''.join(article_link.text.split())\n",
        "            all_titles.append(article_title)\n",
        "\n",
        "            # Click on the article link to access the post content\n",
        "            try:\n",
        "                post_element = driver.find_element(By.XPATH, f'//*[@id=\"main-area\"]/div/table/tbody/tr[{index}]/td/div/div/a')\n",
        "                post_element.click()\n",
        "                time.sleep(3)  # Wait for the post page to load\n",
        "\n",
        "                # Extract the content of the post\n",
        "                post_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "                post_content = post_soup.select_one('.se-main-container').text if post_soup.select_one('.se-main-container') else \"No content found\"\n",
        "\n",
        "                all_contents.append(post_content)\n",
        "\n",
        "                print(f'Title: {article_title}')\n",
        "                print(f'Content: {post_content[:100]}...')  # Display first 100 characters as a sample\n",
        "\n",
        "                driver.back()  # Go back to the main list\n",
        "                time.sleep(3)\n",
        "                switch_to_cafe_frame()  # Switch back to the 'cafe_main' frame\n",
        "            except NoSuchElementException:\n",
        "                print(f\"Unable to click or access post at index {index}. Skipping...\")\n",
        "\n",
        "    # Navigate to the next page\n",
        "    if i < 2:\n",
        "        try:\n",
        "            next_page_button = driver.find_element(By.XPATH, f'//*[@id=\"main-area\"]/div/a[contains(@class,\"next\")]')\n",
        "            next_page_button.click()\n",
        "            time.sleep(3)\n",
        "            switch_to_cafe_frame()\n",
        "        except NoSuchElementException:\n",
        "            print(\"No next page button found. Exiting...\")\n",
        "            break\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df = pd.DataFrame({'Title': all_titles, 'Content': all_contents})\n",
        "df.to_csv(\"crawled_posts.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"Crawling completed and saved to 'crawled_posts.csv'\")"
      ],
      "metadata": {
        "id": "WJhkrQ93oCYA",
        "outputId": "897cd6e7-1185-4d90-d118-fc8999732a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully switched to the 'cafe_main' frame\n",
            "---- Page 1 ----\n",
            "Title: 작명소추천미즈아가행복작명연구원후기\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 서울시행복수유는출산후신청하는거죠?\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 코코하니가을옷(내복,가디건,후드티,팬츠등등)만원의행복할인중이요오\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: [팝니다]행복한성교육동화\n",
            "Content: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "책기둥 빛바램 없이 깨끗합니다낙서나 오염 없고 상태 좋아요문자나 쳇 주세요\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "​​\n",
            "\n",
            "\n",
            "\n",
            " ...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 인천서구청라산후도우미\"행복맘스\"후기^_^\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 시간제보육결제요!국민행복카드바우처금액이없어도사용가능한가요?ㅜㅜ\n",
            "Content: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 지금 보니 국민 행복카드로만 결제가 되는 것 같은데 바우처 금액이 없어도 그 카드로해도  결제되는 게 맞나요?ㅜㅜ​\n",
            "\n",
            "\n",
            "\n",
            " ...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: \"엄마에게어떤말을들었을때가장행복해?\"\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 행복애개육아\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 강서구작명소추천미즈아가행복작명연구원후기\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: [육아용품]만원의행복)돗투돗뽀송양털목도리,쁘띠목도리10,000원핫딜\n",
            "Content: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "털목도리랑 양털목도리 할인해요!!!주문제작이라 목도리 할 날씨 전까지는 오겠네요ㅎㅎ겨울꺼는 지금 사두는게 싸더라구요 미리 주문해두세요~돗프렌즈 자수라 넘 귀엽네요ㅎㅎ​ht...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 이혼하고행복하게\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 국민행복카드발급+임신출산진료비바우처신청\n",
            "Content: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "​​이미 진료비 바우처는 신청을 했었고오늘 삼성카드에서 카드 발급받았어요! (아직 미수령)​이제 카드갖고 병원가서 바우처쓰겠다 말하고 코드입력하면 바우처 쓰는거 맞는거죠?...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 국민행복카드타은행재발급하려고하는데\n",
            "Content: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "제가 원래 삼성카드 쓰고 있다가삼성카드 신용카드를 없애고 신한카드를 만드려고해서국민행복카드도 신한은행으로 갈아타려고하는데 그럼 어떡게하면 될까요?​어디에 등록해야하나요? ...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 미즈아가행복작명연구원방문후기!\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "Title: 국민행복카드기업은행신용카드발급\n",
            "Content: No content found...\n",
            "Successfully switched to the 'cafe_main' frame\n",
            "No next page button found. Exiting...\n",
            "Crawling completed and saved to 'crawled_posts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchFrameException, NoSuchElementException\n",
        "\n",
        "# Initialize the Selenium driver function\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome path\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line if you want to see the browser\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Start the driver\n",
        "driver = initialize_driver()\n",
        "\n",
        "# Log in to Naver\n",
        "driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
        "time.sleep(2)\n",
        "driver.find_element(By.ID, \"id\").send_keys(\"postpia\")  # Replace with your Naver ID\n",
        "driver.find_element(By.ID, \"pw\").send_keys(\"na700925!shj\")  # Replace with your Naver password\n",
        "driver.find_element(By.CLASS_NAME, \"btn_login\").click()\n",
        "time.sleep(5)  # Wait for login to complete\n",
        "\n",
        "# Navigate to the page where you want to scrape articles\n",
        "driver.get(\"https://section.cafe.naver.com/ca-fe/home/search/articles?q=육아&includeAll=&exclude=&t=1727425435248&em=1\")\n",
        "time.sleep(5)  # Allow page to load completely\n",
        "\n",
        "# Initialize lists to store titles, content, and links\n",
        "all_titles = []\n",
        "all_contents = []\n",
        "all_links = []\n",
        "\n",
        "# Loop through pages (for demonstration, scraping the first 3 pages only)\n",
        "for i in range(1, 4):\n",
        "    req = driver.page_source\n",
        "    soup = BeautifulSoup(req, 'html.parser')\n",
        "\n",
        "    # Locate the article elements\n",
        "    articles = soup.select(\"div.ArticleItem\")\n",
        "\n",
        "    print(f'---- Page {i} ----')\n",
        "    print(f\"Found {len(articles)} articles on page {i}\")\n",
        "\n",
        "    for article in articles:\n",
        "        # Extract the title\n",
        "        title_element = article.select_one('strong.title')\n",
        "        article_title = title_element.get_text(strip=True) if title_element else \"No title found\"\n",
        "\n",
        "        # Extract the link\n",
        "        link_element = article.find('a', href=True)\n",
        "        article_link = link_element['href'] if link_element else \"No link found\"\n",
        "\n",
        "        # Extract the content\n",
        "        content_element = article.select_one('p.text')\n",
        "        article_content = content_element.get_text(strip=True) if content_element else \"No content found\"\n",
        "\n",
        "        # Add the extracted data to the lists\n",
        "        all_titles.append(article_title)\n",
        "        all_contents.append(article_content)\n",
        "        all_links.append(article_link)\n",
        "\n",
        "        print(f'Title: {article_title}')\n",
        "        print(f'Content: {article_content[:200]}...')  # Display first 100 characters as a sample\n",
        "        print(f'Link: {article_link}')\n",
        "\n",
        "    # Navigate to the next page if there's a button/link\n",
        "    try:\n",
        "        next_page_button = driver.find_element(By.XPATH, f'//*[@id=\"main-area\"]/div/a[contains(@class,\"next\")]')\n",
        "        next_page_button.click()\n",
        "        time.sleep(3)\n",
        "    except NoSuchElementException:\n",
        "        print(\"No next page button found. Exiting...\")\n",
        "        break\n",
        "\n",
        "# Save the results to a CSV file\n",
        "df = pd.DataFrame({'Title': all_titles, 'Content': all_contents, 'Link': all_links})\n",
        "df.to_csv(\"crawled_posts.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"Crawling completed and saved to 'crawled_posts.csv'\")"
      ],
      "metadata": {
        "id": "1spRUx-Oj6jh",
        "outputId": "8baf82ed-4286-47b4-f17a-074876a1cf97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Page 1 ----\n",
            "Found 12 articles on page 1\n",
            "Title: 육아용품리스트 신생아육아템 이건 꼭!!\n",
            "Content: 육아용품리스트 중에서도 특히나 신생아육아템은 고민이 많이 되잖아요육아에 미숙하기도 하고 아이 면역력이 제일 약한 시기에 쓰게 되는게 신생아육아템이니 편하면서도......\n",
            "Link: https://cafe.naver.com/andongmom?iframe_url=%2FArticleRead.nhn%3Fclubid%3D24381989%26articleid%3D796781%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3OTY3ODEsImNhZmVVcmwiOiJhbmRvbmdtb20iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTAzNH0.XJumY_iu9-lL387GlL5mjmY3tJ0U3U-MgFgx01zDvuE%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 자영업자육아휴직 이렇게!\n",
            "Content: 자영업자육아휴직 이렇게! 자영업자육아휴직알아보시나요? 자영업자 여러분, 기쁜 소식이 있습니다! 정부가 2024년부터 자영업자를 위한육아휴직 제도를 도입할 예정입니다.......\n",
            "Link: https://cafe.naver.com/chororocontest?iframe_url=%2FArticleRead.nhn%3Fclubid%3D20093422%26articleid%3D232450%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMzI0NTAsImNhZmVVcmwiOiJjaG9yb3JvY29udGVzdCIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDM3fQ.oAPjnptsHk9GptTLY8AgOYRLR7Ov7_1y0rakz20Jthk%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 육아휴직 1년 6개월 연장 확정!\n",
            "Content: 그러나 일괄 적용은 아니고 부, 모가 각각육아휴직을 3개월 이상 사용했을 경우, 또는... 1년의육아단축근무기간이 남아 있는데요. 육휴 자체보다는 그로 인한 단축근무기간......\n",
            "Link: https://cafe.naver.com/mapomommy?iframe_url=%2FArticleRead.nhn%3Fclubid%3D18087572%26articleid%3D1316336%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMzE2MzM2LCJjYWZlVXJsIjoibWFwb21vbW15IiwiaXNzdWVkQXQiOjE3Mjc3ODE5NDEwMzl9.kEdkmpb4tOmvAjBxyZ5nOkWyMXvs4UICxKqFMLueA8Q%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 공무원육아휴직 급여와 기간 2024년 개정사항 정리 (+육아휴직 공제)\n",
            "Content: 공무원이 유독 맘시생이나 주부들에게 인기있는 이유는 공무원육아휴직 급여 때문이라고 볼 수 있습니다. 사기업의 경우육아휴직이 제대로 보장되어 있지 않는 경우도 많고......\n",
            "Link: https://cafe.naver.com/m2school?iframe_url=%2FArticleRead.nhn%3Fclubid%3D12026840%26articleid%3D4903150%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo0OTAzMTUwLCJjYWZlVXJsIjoibTJzY2hvb2wiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0MX0.wjYtW_sjh2ysOdZuqIC-plbBiU81Hk5IGg8ztjukcHM%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 2025육아휴직 급여 자세히!\n",
            "Content: 이 글은 카페와 협의된 글입니다 2025육아휴직 급여 자세히! 2025육아휴직 급여 알아보시죠? 2025육아휴직 급여 이란 2025년 1월 1일부터육아휴직 급여가 월 150만 원에서 250만......\n",
            "Link: https://cafe.naver.com/mokpomam?iframe_url=%2FArticleRead.nhn%3Fclubid%3D13584633%26articleid%3D2228360%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMjI4MzYwLCJjYWZlVXJsIjoibW9rcG9tYW0iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0M30.UMVvoZykgigNLNMfqIM6vqKIUPkdi0ICGVq2ju7YGVw%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 6+6육아휴직 급여 궁금해요!\n",
            "Content: 지난 3월에 출산하고 현재육아휴직 중인데, 6+6육아휴직 급여 제도에 대해 궁금한 점이 너무 많아서 글을 올려봅니다. 우선 6+6육아휴직 급여 제도가 정확히 어떤 건지 잘......\n",
            "Link: https://cafe.naver.com/peopledisc?iframe_url=%2FArticleRead.nhn%3Fclubid%3D24885887%26articleid%3D857654%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo4NTc2NTQsImNhZmVVcmwiOiJwZW9wbGVkaXNjIiwiaXNzdWVkQXQiOjE3Mjc3ODE5NDEwNDZ9.2hrtcUyGFVNhxYN4OT_WLuk8ziPbmRENxx4XUg_Ck-k%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 2025육아휴직 급여 공무원육아휴직 인상된 급여는?\n",
            "Content: 오늘은 2025육아휴직 급여에 대한 정보를 소개해 드리겠습니다. 공무원육아휴직을 하는 동안육아휴직 급여를 받게 되는데요. 특히 2025육아휴직 급여가 인상되었기......\n",
            "Link: https://cafe.naver.com/gugrade?iframe_url=%2FArticleRead.nhn%3Fclubid%3D11570974%26articleid%3D11523053%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMTUyMzA1MywiY2FmZVVybCI6Imd1Z3JhZGUiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0OH0.DodpPUTS9yaMoWJhEUl77S9rdQhJeR4tw8eyaIP-mCI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 육아휴직 1년6개월 궁금합니다ㅠㅠ\n",
            "Content: 제가 3월부터 산전육아휴직시작하고 내년6월 복직예정입니다! 남편이 올8월부터육아휴직들어가 같이 6+6제도육아휴직중인데 법개정되어 6개월 연장되었다고 내년2월부터......\n",
            "Link: https://cafe.naver.com/imsanbu?iframe_url=%2FArticleRead.nhn%3Fclubid%3D10094499%26articleid%3D72592192%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3MjU5MjE5MiwiY2FmZVVybCI6Imltc2FuYnUiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1MH0.5zOX0WEVwBYOrRBdVfoAsD3S1R8dC38n1jCbEwHwdWI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 육아휴직 사후지급금 폐지, 인상 총정리\n",
            "Content: 육아휴직 사후지급금 폐지, 인상 총정리 2025년부터육아휴직 급여가 인상된다는 소식이 전해지면서 임신과 출산을 앞둔 많은 분들의 관심이 쏠리고 있습니다. 이번......\n",
            "Link: https://cafe.naver.com/rksghwhantk?iframe_url=%2FArticleRead.nhn%3Fclubid%3D10208198%26articleid%3D1191420%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMTkxNDIwLCJjYWZlVXJsIjoicmtzZ2h3aGFudGsiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1Mn0.58qWaxMvOQ8qEsLQoG6NhZ9UqxO_FzGxjHKv0eMQx2g%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 2025육아휴직 급여 인상? 변경된 사항 총정리!!\n",
            "Content: 2025육아휴직 급여 알아보시나요? 고용노동부가 '2025년 예산안'이 의결되었다고 밝혔는데요!육아휴직 급여 인상이 내년 예산에 포함되었다는 소식이 있어서요! 빠르게......\n",
            "Link: https://cafe.naver.com/ungsangjang?iframe_url=%2FArticleRead.nhn%3Fclubid%3D26334430%26articleid%3D726580%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3MjY1ODAsImNhZmVVcmwiOiJ1bmdzYW5namFuZyIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDU1fQ.thE2aFJKIl_z47QnmQ9aYfgNOGQkldq3eTCBnvIKPZ4%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 육아휴직 1년 6개월 자세히!\n",
            "Content: 이 글은 카페와 협의된 글입니다육아휴직 1년 6개월 자세히!육아휴직 1년 6개월 알아보시죠?육아휴직 1년 6개월이란 출산 또는 입양 후 근로자가 급여를 받으며 일시적으로......\n",
            "Link: https://cafe.naver.com/mokpomam?iframe_url=%2FArticleRead.nhn%3Fclubid%3D13584633%26articleid%3D2227399%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMjI3Mzk5LCJjYWZlVXJsIjoibW9rcG9tYW0iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1N30.4eqwSrGlKnhX4nmmOwpW7d7bV86o8PBBR5kqKY506jI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 육아휴직 급여 내용 1년 6개월 6+6 총정리\n",
            "Content: 육아휴직 급여 내용 1년 6개월 6+6 총정리육아휴직 급여 내용 알아보시죠? 2024년부터 시행되는 6+6 부모육아휴직제도는 부모가 동시에 또는 순차적으로육아휴직을 사용할......\n",
            "Link: https://cafe.naver.com/chororocontest?iframe_url=%2FArticleRead.nhn%3Fclubid%3D20093422%26articleid%3D232062%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMzIwNjIsImNhZmVVcmwiOiJjaG9yb3JvY29udGVzdCIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDU5fQ.Tc16o0r_Q-kjl0X9V2vJ9OnDZHRqm_oua0BtAsv4eaY%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "No next page button found. Exiting...\n",
            "Crawling completed and saved to 'crawled_posts.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Initialize the Selenium driver function\n",
        "def initialize_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'  # Chrome path\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--headless\")  # Comment this line if you want to see the browser\n",
        "    driver = webdriver.Chrome(service=Service('/usr/local/bin/chromedriver'), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Start the driver\n",
        "driver = initialize_driver()\n",
        "# Log in to Naver\n",
        "driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
        "time.sleep(2)\n",
        "driver.find_element(By.ID, \"id\").send_keys(\"postpia\")  # Replace with your Naver ID\n",
        "driver.find_element(By.ID, \"pw\").send_keys(\"na700925!shj\")  # Replace with your Naver password\n",
        "driver.find_element(By.CLASS_NAME, \"btn_login\").click()\n",
        "time.sleep(5)  # Wait for login to complete\n",
        "# Load the uploaded CSV file to see the structure and extract link addresses\n",
        "file_path = '/content/crawled_posts.csv'\n",
        "df_new_links = pd.read_csv(file_path)\n",
        "df_new_links['Category']= 'N/A'\n",
        "# Display the first few rows to understand the structure\n",
        "df_new_links.head()\n",
        "\n",
        "# Extracting the URLs from the 'Link' column for the new crawling task\n",
        "new_urls = df_new_links['Link'].dropna().unique()  # Ensuring unique and non-empty links\n",
        "\n",
        "# Function to crawl content with enhanced handling for encoding\n",
        "def extract_content_with_selectors(url):\n",
        "    try:\n",
        "        # Initialize the driver and navigate to the URL\n",
        "        driver.get(url)\n",
        "        time.sleep(3)  # Wait for the page to load\n",
        "\n",
        "        # Switch to the main frame containing the content\n",
        "        driver.switch_to.frame(\"cafe_main\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Parse the page source with BeautifulSoup\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        # Extract category\n",
        "        category = soup.select_one(\"a.link_board\")\n",
        "\n",
        "        if category:\n",
        "          category = category.get_text(strip=True).split(' ')[0]\n",
        "        else:\n",
        "          category = \"N/A\"\n",
        "\n",
        "        span_elements = soup.select(\"span.se-fs-.se-ff-\")\n",
        "        content = \"\\n\".join([element.get_text(strip=True) for element in span_elements])\n",
        "\n",
        "        # Extract title using a general title tag\n",
        "        title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
        "        return title, content,category if content else \"No Content Found\"\n",
        "    except Exception as e:\n",
        "        return \"Error\", str(e)\n",
        "\n",
        "# Reinitialize the driver for this extraction attempt\n",
        "driver = initialize_driver()\n",
        "\n",
        "# Extract content using the updated function\n",
        "titles_updated = []\n",
        "contents_updated = []\n",
        "categories_updated = []\n",
        "for url in new_urls:\n",
        "    title, content,category = extract_content_with_selectors(url)\n",
        "    titles_updated.append(title)\n",
        "    contents_updated.append(content)\n",
        "    categories_updated.append(category)\n",
        "# Close the driver after extraction\n",
        "driver.quit()\n",
        "\n",
        "# Display the extracted results in a simple text format\n",
        "for idx, (url, title, content,category) in enumerate(zip(new_urls, titles_updated, contents_updated,categories_updated), start=1):\n",
        "    print(f\"\\n### Post {idx} ###\")\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Content:  {content[:500]}...\")   # Displaying the first 500 characters of the content for brevity\n",
        "    print(f\"Category: {category}\")\n",
        "# Create a DataFrame from the new data\n",
        "df_new_content = pd.DataFrame({\n",
        "    'URL': new_urls,\n",
        "    'Title': titles_updated,\n",
        "    'Content': contents_updated,\n",
        "    'Category': categories_updated\n",
        "})\n",
        "\n",
        "# Ensure the URLs match correctly between the existing and new data\n",
        "df_new_links['Content'] = df_new_links['Link'].map(df_new_content.set_index('URL')['Content'])\n",
        "df_new_links['Category'] = df_new_links['Link'].map(df_new_content.set_index('URL')['Category'])\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df_new_links.to_csv('/content/crawled_results.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "MqAkDjkE9vzY",
        "outputId": "aa57718e-03df-412a-ee14-c6d2ad1c0d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Post 1 ###\n",
            "URL: https://cafe.naver.com/andongmom?iframe_url=%2FArticleRead.nhn%3Fclubid%3D24381989%26articleid%3D796781%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3OTY3ODEsImNhZmVVcmwiOiJhbmRvbmdtb20iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTAzNH0.XJumY_iu9-lL387GlL5mjmY3tJ0U3U-MgFgx01zDvuE%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  육아용품리스트 중에서도\n",
            "특히나 신생아육아템은\n",
            "고민이 많이 되잖아요\n",
            "​\n",
            "육아에 미숙하기도 하고\n",
            "아이 면역력이 제일 약한 시기에\n",
            "쓰게 되는게 신생아육아템이니\n",
            "편하면서도 위생적인 제품을\n",
            "선택해 주는게 좋아요\n",
            "​\n",
            "저는 그래서 육아용품리스트를\n",
            "작성할 때 분유포트는 꼭!\n",
            "꼼꼼하게 찾아보고 비교를 해본 뒤\n",
            "선택을 하라고 해요~\n",
            "울집 맘마존을 책임지고 있는 아이는\n",
            "UV살균이 가능한 모델\n",
            "이에요\n",
            "​\n",
            "다들 아시겠지만\n",
            "물을 한번 끓였더라도\n",
            "\n",
            "체온정도의 온도로 보온을 하게되면\n",
            "세균이 빠른 속도로 번식\n",
            "하게 되거든요\n",
            "그런데 생각보다 세균 번식 속도가\n",
            "빨라서 이 물로 분유를 타주면\n",
            "아이 배앓이를 유발\n",
            "할 수 있기 때문에\n",
            "특히나 신생아육아템으로 고민을\n",
            "하는 분들이라면\n",
            "​\n",
            "UV기능이 추가로 있는 모델을\n",
            "쓰는게 좋아요\n",
            "​\n",
            "​\n",
            "또, 요즘은 출수형이 잘 나와서\n",
            "손으로 맞춰주는 것 보다\n",
            "훨씬 정확해요 ㅎㅎ\n",
            "육아용품리스트로 추천을 받아도\n",
            "예전 제품들 오차가 있는 편이라\n",
            "완전히 믿고 쓰지를 못했었거든요\n",
            "​\n",
            "제가 쓰고 있는...\n",
            "Category: 궁금한건물어봐~\n",
            "\n",
            "### Post 2 ###\n",
            "URL: https://cafe.naver.com/chororocontest?iframe_url=%2FArticleRead.nhn%3Fclubid%3D20093422%26articleid%3D232450%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMzI0NTAsImNhZmVVcmwiOiJjaG9yb3JvY29udGVzdCIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDM3fQ.oAPjnptsHk9GptTLY8AgOYRLR7Ov7_1y0rakz20Jthk%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 3 ###\n",
            "URL: https://cafe.naver.com/mapomommy?iframe_url=%2FArticleRead.nhn%3Fclubid%3D18087572%26articleid%3D1316336%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMzE2MzM2LCJjYWZlVXJsIjoibWFwb21vbW15IiwiaXNzdWVkQXQiOjE3Mjc3ODE5NDEwMzl9.kEdkmpb4tOmvAjBxyZ5nOkWyMXvs4UICxKqFMLueA8Q%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 4 ###\n",
            "URL: https://cafe.naver.com/m2school?iframe_url=%2FArticleRead.nhn%3Fclubid%3D12026840%26articleid%3D4903150%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo0OTAzMTUwLCJjYWZlVXJsIjoibTJzY2hvb2wiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0MX0.wjYtW_sjh2ysOdZuqIC-plbBiU81Hk5IGg8ztjukcHM%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 5 ###\n",
            "URL: https://cafe.naver.com/mokpomam?iframe_url=%2FArticleRead.nhn%3Fclubid%3D13584633%26articleid%3D2228360%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMjI4MzYwLCJjYWZlVXJsIjoibW9rcG9tYW0iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0M30.UMVvoZykgigNLNMfqIM6vqKIUPkdi0ICGVq2ju7YGVw%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 6 ###\n",
            "URL: https://cafe.naver.com/peopledisc?iframe_url=%2FArticleRead.nhn%3Fclubid%3D24885887%26articleid%3D857654%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo4NTc2NTQsImNhZmVVcmwiOiJwZW9wbGVkaXNjIiwiaXNzdWVkQXQiOjE3Mjc3ODE5NDEwNDZ9.2hrtcUyGFVNhxYN4OT_WLuk8ziPbmRENxx4XUg_Ck-k%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  안녕하세요, 6개월 된 아기 엄마예요.\n",
            "​\n",
            "지난 3월에 출산하고 현재 육아휴직 중인데,\n",
            "6+6 육아휴직 급여 제도에 대해\n",
            "궁금한 점이 너무 많아서 글을 올려봅니다.\n",
            "​\n",
            "​\n",
            "우선 6+6 육아휴직 급여 제도가\n",
            "정확히 어떤 건지 잘 모르겠어요.\n",
            "​\n",
            "제가 알기로는 부모가 동시에 또는 순차적으로 육아휴직을 사용할 때\n",
            "각각 6개월씩 통상임금의 100%를 지원받는다고 하던데, 맞나요?\n",
            "​\n",
            "그리고 이 제도를 이용하려면\n",
            "어떤 조건을 충족해야 하는지 궁금해요.\n",
            "​\n",
            "육아휴직 급여 금액 계산이\n",
            "좀 복잡한 것 같아요.\n",
            "​\n",
            "6개월 동안은 통상임금의 100%를 받고,\n",
            "나머지 6개월은 통상임금의 50%를 받는 건가요?\n",
            "​\n",
            "그리고 상한액과 하한액이 있다고 들었는데,\n",
            "정확히 얼마인가요?\n",
            "​\n",
            "또 6+6 육아휴직 급여를 받으면서\n",
            "다른 소득 활동을 해도 되는지 궁금해요.\n",
            "​\n",
            "재택으로 할 수 있는 아르바이트나\n",
            "프리랜서 일을 하고 싶은데,\n",
            "이렇게 하면 육아휴직 급여에 영향이 있나요?\n",
            "​\n",
            "그리고 6+6 육아휴직 기간을\n",
            "나눠서...\n",
            "Category: 자유게시판\n",
            "\n",
            "### Post 7 ###\n",
            "URL: https://cafe.naver.com/gugrade?iframe_url=%2FArticleRead.nhn%3Fclubid%3D11570974%26articleid%3D11523053%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMTUyMzA1MywiY2FmZVVybCI6Imd1Z3JhZGUiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA0OH0.DodpPUTS9yaMoWJhEUl77S9rdQhJeR4tw8eyaIP-mCI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 8 ###\n",
            "URL: https://cafe.naver.com/imsanbu?iframe_url=%2FArticleRead.nhn%3Fclubid%3D10094499%26articleid%3D72592192%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3MjU5MjE5MiwiY2FmZVVybCI6Imltc2FuYnUiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1MH0.5zOX0WEVwBYOrRBdVfoAsD3S1R8dC38n1jCbEwHwdWI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 9 ###\n",
            "URL: https://cafe.naver.com/rksghwhantk?iframe_url=%2FArticleRead.nhn%3Fclubid%3D10208198%26articleid%3D1191420%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoxMTkxNDIwLCJjYWZlVXJsIjoicmtzZ2h3aGFudGsiLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1Mn0.58qWaxMvOQ8qEsLQoG6NhZ9UqxO_FzGxjHKv0eMQx2g%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ​\n",
            "2025년부터 육아휴직 급여가 인상된다는 소식이 전해지면서 임신과 출산을 앞둔 많은 분들의 관심이 쏠리고 있습니다. 이번 고용노동부 발표에 따르면, 육아휴직 급여 상한액이 인상되고, 사후지급금도 폐지된다고 하니, 관련된 내용을 자세히 알아보겠습니다.\n",
            "​\n",
            "먼저, 육아휴직 급여 상한액이 인상됩니다. 기존에는 육아휴직 급여가 1년 동안 통상임금의 80%였으며, 상한액이 150만 원, 하한액이 70만 원으로 정해져 있었습니다. 하지만 사후지급금이라는 제도로 25%를 나중에 받는 방식이었죠. 예를 들어 급여가 300만 원이었던 사람이 육아휴직을 하면 상한액 150만 원에서 25%를 뗀 112만 5천 원을 실수령하게 되는 구조였습니다. 이는 물가 상승에도 불구하고 변함없이 유지되어 경제적인 부담을 주었습니다.\n",
            "​\n",
            "하지만 2025년 1월 1일부터는 이런 문제가 개선됩니다. 인상된 급여 구조는 다음과 같습니다. 13개월 차에는 통상임금의 100%를 지급하며, 상한액은 250만 원입니다. 46개...\n",
            "Category: 👩간호人｜자유수다\n",
            "\n",
            "### Post 10 ###\n",
            "URL: https://cafe.naver.com/ungsangjang?iframe_url=%2FArticleRead.nhn%3Fclubid%3D26334430%26articleid%3D726580%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjo3MjY1ODAsImNhZmVVcmwiOiJ1bmdzYW5namFuZyIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDU1fQ.thE2aFJKIl_z47QnmQ9aYfgNOGQkldq3eTCBnvIKPZ4%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  2025 육아휴직 급여 알아보시나요?\n",
            "​\n",
            "고용노동부가 '2025년 예산안'이 의결되었다고 밝혔는데요! 육아휴직 급여 인상이 내년 예산에 포함되었다는 소식이 있어서요!\n",
            "​\n",
            "빠르게 정리해서 여러분이 궁금하신 사항만 쏙쏙 전달드릴게요!\n",
            "​\n",
            "바로 아래의\n",
            "2025 변경된 육아휴직 급여 정보 바로가기를 누르셔서 빠르게 정보 확인해보세요.\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "​\n",
            "고용노동부는 2023년 7월 19일 '고용보험법' 및 '고용산재보험료징수법' 시행령 일부 개정안을 입법예고했습니다.\n",
            "​\n",
            "이번 개정안에서는 부모 모두 3개월+3개월 육아휴직 제도가 신설되며,\n",
            "​\n",
            "육아휴직 급여 소득대체율 인상 계획도 발표되었습니다.\n",
            "​\n",
            "아래는 2025 육아휴직 급여 주요 내용입니다.\n",
            "​\n",
            "● 부모 모두 3개월 + 3개월 육아휴직제 신설\n",
            "​\n",
            "- 생후 12개월 이내 자녀에 대해 부모가 동시에 또는 순차적으로 육아휴직 사용 시, 첫 3개월에 대해 부모 각각 통상임금의 100% 지원 (상한액 월 300...\n",
            "Category: 🌈시끌벅적\n",
            "\n",
            "### Post 11 ###\n",
            "URL: https://cafe.naver.com/mokpomam?iframe_url=%2FArticleRead.nhn%3Fclubid%3D13584633%26articleid%3D2227399%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMjI3Mzk5LCJjYWZlVXJsIjoibW9rcG9tYW0iLCJpc3N1ZWRBdCI6MTcyNzc4MTk0MTA1N30.4eqwSrGlKnhX4nmmOwpW7d7bV86o8PBBR5kqKY506jI%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n",
            "\n",
            "### Post 12 ###\n",
            "URL: https://cafe.naver.com/chororocontest?iframe_url=%2FArticleRead.nhn%3Fclubid%3D20093422%26articleid%3D232062%26art%3DaW50ZXJuYWwtY2FmZS13ZWItc2VjdGlvbi1zZWFyY2gtbGlzdA.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfVVJMIiwiYXJ0aWNsZUlkIjoyMzIwNjIsImNhZmVVcmwiOiJjaG9yb3JvY29udGVzdCIsImlzc3VlZEF0IjoxNzI3NzgxOTQxMDU5fQ.Tc16o0r_Q-kjl0X9V2vJ9OnDZHRqm_oua0BtAsv4eaY%26query%3D%25EC%259C%25A1%25EC%2595%2584\n",
            "Title: 네이버 카페\n",
            "Content:  ...\n",
            "Category: No Content Found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}